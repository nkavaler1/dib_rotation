{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to your DIB-lab rotation project! All resources needed to learn a metagenomics workflow are contained within this repository. If you see a mistake or something is not clear, please submit an issue . During this rotation, you will learn how to: interact with an HPC (we'll use Farm ) install and manage software environments using conda download sequencing data and other files from the internet and public databases interpret and use different file formats in bioinformatics and computing quality analysis and control for sequencing data determine the taxonomic composition of sequencing reads assemble and annotate metagenomic reads quickly compare large sequencing datasets document workflows using git and GitHub. The files in this repository are ordered by execution, meaning file 00* should be completed before 01* . Most of the work done in this rotation will be completed on Farm. However, you will need to access Farm from your own computer. We will use an SSH-client to be able to interact with Farm. If you are using a Mac or a Linux, your computer comes with a program called Terminal that we will use as an SSH-client. If you are on a Windows running Windows 10, you can install the Ubuntu Subsystem . Otherwise, please follow the instructions for Windows found at this link .","title":"Home"},{"location":"00_getting_started/","text":"Welcome to your DIB-lab rotation project! All resources needed to learn a metagenomics workflow are contained within this repository. If you see a mistake or something is not clear, please submit an issue . During this rotation, you will learn how to: + interact with an HPC (we'll use Farm ) + install and manage software environments using conda + download sequencing data and other files from the internet and public databases + interpret and use different file formats in bioinformatics and computing + quality analysis and control for sequencing data + determine the taxonomic composition of sequencing reads + assemble and annotate metagenomic reads + quickly compare large sequencing datasets + document workflows using git and GitHub. The files in this repository are ordered by execution, meaning file 00* should be completed before 01* . Most of the work done in this rotation will be completed on Farm. However, you will need to access Farm from your own computer. We will use an SSH-client to be able to interact with Farm. If you are using a Mac or a Linux, your computer comes with a program called Terminal that we will use as an SSH-client. If you are on a Windows running Windows 10, you can install the Ubuntu Subsystem . Otherwise, please follow the instructions for Windows found at this link .","title":"Home"},{"location":"00a_unix/","text":"Here are some Unix tutorials you can use to familiarize yourself with linux command line: + Introduction to Command Line for Genomics . You will need to download the data for this lesson here . + An Introduction to Shell . + The Unix Shell + Introduction to using the shell in a High-Performance Computing context","title":"00a unix"},{"location":"01_farm_account/","text":"High Performance Computing (HPC) refers to computers that have more capability than a typical personal computer (i.e. most desktops and laptops). Many research problems we encounter when analyzing sequencing data require more resources than we have available on our laptops. For this, we use large, remote compute systems that have more resources available. Most universities have access to an HPC (or cluster) that has a large amount of hard drive space to store files, RAM for computing tasks, and CPUs for processing. Other options for accessing large computers include NSF XSEDE services like Jetstream and paid services like Amazon Web Services or Google Cloud. We will use the UC Davis Farm Cluster during this rotation. Getting an account on Farm To be able to use Farm, you need to sign up for an account. Farm requires key file authentication. Key files come in pairs like the locks and keys on doors. The private key file is the first file, and it is like the key to a door. This file is private and should never be shared with anyone (do not post this file on GitHub, slack, etc.). The public key file is the second file, and it is like the lock on a door. It is publicly viewable, but cannot be \"unlocked\" without the private key file. We need to generate a key file pair in order to create a farm account. Open the Terminal application or the Terminal emulator you installed in the first_lesson . Change directories into the .ssh folder. This folder is where key file pairs are typically stored. cd ~/.ssh If this command does not work, create your own ssh folder and cd into it: mkdir -p ~/.ssh cd ~/.ssh Then, generate the keyfile pair by running: ssh-keygen Follow the prompts on the screen. If prompted for a password, you can hit Enter on your keyboard to avoid setting one. Two files will be created by this command. These files should have the same prefix. The file that ends in .pub is the public key. The account request form Next, navigate to this page . From the first drop down menu (Which cluster are you applying for an account on?), select FARM/CAES . From the second drop down menu (Who is sponsoring your account?), select Brown, C. Titus . Then, upload your public key file to the page. Submit the form. If the cluster admins and Titus approve your account, you will now have farm access! Don't loose the key file pair you just made. You will need the private key file each time you log into farm. Connecting to a remote computer Once you have a farm account, we will use the command ssh to connect to farm. ssh stands for \"secure shell\". To connect to your account on farm, type: ssh -i ~/.ssh/your_keyfile_name username@farm.cse.ucdavis.edu If you are successful, you will see a message that looks something like this: Welcome to Ubuntu 18.04.3 LTS (GNU/Linux 4.15.0-70-generic x86_64) 1 updates could not be installed automatically. For more details, see /var/log/unattended-upgrades/unattended-upgrades.log *** System restart required *** A transfer node, c11-42, is available for rsync, scp, gzip From outside the Farm cluster use port 2022 to access the transfer node. ssh -p 2022 username@farm.cse.ucdavis.edu scp -P 2022 src username@farm.cse.ucdavis.edu:/destination REMINDER: Farm does not back up user data. Please ensure your data is backed up offsite. *** Dec 04 2019: * 2:10pm - Service restored. Please report any issues to help@cse.ucdavis.edu. Email help@cse.ucdavis.edu for help with Farm. Downtime scheduled for the first Wednesday of Oct and April. The next downtime is Wednesday April 1st at 11:59pm. If interested in contributing to farm, the rates for 5 years are: $ 1,000 per 10TB, served from redundant servers with compression $ 8,800 per parallel node (256GB ram, 32 cores/64 threads, 2TB /scratch) $17,500 per GPU node (Nvidia Telsa V100, dual Xeon 4114, 2TB /scratch) $22,700 per bigmem node (1TB ram, 48 cores/96 threads, 2TB /scratch) Last login: Thu Jan 2 17:01:36 2020 from 76.105.143.194 Module slurm/19.05.3 loaded Module openmpi/4.0.1 loaded username@farm:~$ When you first login to farm, you will be in your home directory. This is where you will write your files and run the majority of your commands. When you are done using farm, you can exit your ssh connection with the exit command. exit","title":"Setting up Farm"},{"location":"01_farm_account/#getting-an-account-on-farm","text":"To be able to use Farm, you need to sign up for an account. Farm requires key file authentication. Key files come in pairs like the locks and keys on doors. The private key file is the first file, and it is like the key to a door. This file is private and should never be shared with anyone (do not post this file on GitHub, slack, etc.). The public key file is the second file, and it is like the lock on a door. It is publicly viewable, but cannot be \"unlocked\" without the private key file. We need to generate a key file pair in order to create a farm account. Open the Terminal application or the Terminal emulator you installed in the first_lesson . Change directories into the .ssh folder. This folder is where key file pairs are typically stored. cd ~/.ssh If this command does not work, create your own ssh folder and cd into it: mkdir -p ~/.ssh cd ~/.ssh Then, generate the keyfile pair by running: ssh-keygen Follow the prompts on the screen. If prompted for a password, you can hit Enter on your keyboard to avoid setting one. Two files will be created by this command. These files should have the same prefix. The file that ends in .pub is the public key.","title":"Getting an account on Farm"},{"location":"01_farm_account/#the-account-request-form","text":"Next, navigate to this page . From the first drop down menu (Which cluster are you applying for an account on?), select FARM/CAES . From the second drop down menu (Who is sponsoring your account?), select Brown, C. Titus . Then, upload your public key file to the page. Submit the form. If the cluster admins and Titus approve your account, you will now have farm access! Don't loose the key file pair you just made. You will need the private key file each time you log into farm.","title":"The account request form"},{"location":"01_farm_account/#connecting-to-a-remote-computer","text":"Once you have a farm account, we will use the command ssh to connect to farm. ssh stands for \"secure shell\". To connect to your account on farm, type: ssh -i ~/.ssh/your_keyfile_name username@farm.cse.ucdavis.edu If you are successful, you will see a message that looks something like this: Welcome to Ubuntu 18.04.3 LTS (GNU/Linux 4.15.0-70-generic x86_64) 1 updates could not be installed automatically. For more details, see /var/log/unattended-upgrades/unattended-upgrades.log *** System restart required *** A transfer node, c11-42, is available for rsync, scp, gzip From outside the Farm cluster use port 2022 to access the transfer node. ssh -p 2022 username@farm.cse.ucdavis.edu scp -P 2022 src username@farm.cse.ucdavis.edu:/destination REMINDER: Farm does not back up user data. Please ensure your data is backed up offsite. *** Dec 04 2019: * 2:10pm - Service restored. Please report any issues to help@cse.ucdavis.edu. Email help@cse.ucdavis.edu for help with Farm. Downtime scheduled for the first Wednesday of Oct and April. The next downtime is Wednesday April 1st at 11:59pm. If interested in contributing to farm, the rates for 5 years are: $ 1,000 per 10TB, served from redundant servers with compression $ 8,800 per parallel node (256GB ram, 32 cores/64 threads, 2TB /scratch) $17,500 per GPU node (Nvidia Telsa V100, dual Xeon 4114, 2TB /scratch) $22,700 per bigmem node (1TB ram, 48 cores/96 threads, 2TB /scratch) Last login: Thu Jan 2 17:01:36 2020 from 76.105.143.194 Module slurm/19.05.3 loaded Module openmpi/4.0.1 loaded username@farm:~$ When you first login to farm, you will be in your home directory. This is where you will write your files and run the majority of your commands. When you are done using farm, you can exit your ssh connection with the exit command. exit","title":"Connecting to a remote computer"},{"location":"02_conda/","text":"This tutorial covers the basics of conda including a brief introduction to conda and why it is useful, installation and setup, creating environments, and installing software. These videos cover the material in the above tutorial: + video 1 + video 2 (there were some technical issues with this recording...sorry!) Install miniconda Log in to farm and run the following commands to install Miniconda. Follow the prompts on the screen and accept all default options. wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh bash Miniconda3-latest-Linux-x86_64.sh Miniconda is now installed, but we need to activate it to be able to use it. source ~/.bashrc You should now see (base) in front of your prompt, indicating that you are in the base environment. Configuring channels Channels are the locations of the repositories (directories) online containing Conda packages. Upon Conda\u2019s installation, Continuum\u2019s (Conda\u2019s developer) channels are set by default, so without any further modification, these are the locations where your Conda will start searching for packages. We need to add other channels from which we will be installing software. Channels in Conda are ordered. The channel with the highest priority is the first one that Conda checks, looking for the package you asked for. You can change this order, and also add channels to it (and set their priority as well). If multiple channels contain a package, and one channel contains a newer version than the other one, the order of the channels\u2019 determines which one of these two versions are going to be installed, even if the higher priority channel contains the older version. conda config --add channels defaults conda config --add channels bioconda conda config --add channels conda-forge Creating an environment Now that we have conda installed and our channels configured, we are ready to create an environment. conda create -y -n dib_rotation This creates an empty environment named dib_rotation . To activate this environment, run: conda activate dib_rotation Your prompt should now start with (dib_rotation) . We can now install software into our environment. Let's install sourmash, which we will use in a later lesson. conda install -y sourmash Deactivating and Exiting If you would like to leave your environment, you can type conda deactivate and you will return to the base environment. When you log out of farm by typing exit , when you end a tmux or screen session, or when an srun job ends, your environment will automatically be exited. To restart the environment, you can run conda activate dib_rotation .","title":"Install Conda"},{"location":"02_conda/#install-miniconda","text":"Log in to farm and run the following commands to install Miniconda. Follow the prompts on the screen and accept all default options. wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh bash Miniconda3-latest-Linux-x86_64.sh Miniconda is now installed, but we need to activate it to be able to use it. source ~/.bashrc You should now see (base) in front of your prompt, indicating that you are in the base environment.","title":"Install miniconda"},{"location":"02_conda/#configuring-channels","text":"Channels are the locations of the repositories (directories) online containing Conda packages. Upon Conda\u2019s installation, Continuum\u2019s (Conda\u2019s developer) channels are set by default, so without any further modification, these are the locations where your Conda will start searching for packages. We need to add other channels from which we will be installing software. Channels in Conda are ordered. The channel with the highest priority is the first one that Conda checks, looking for the package you asked for. You can change this order, and also add channels to it (and set their priority as well). If multiple channels contain a package, and one channel contains a newer version than the other one, the order of the channels\u2019 determines which one of these two versions are going to be installed, even if the higher priority channel contains the older version. conda config --add channels defaults conda config --add channels bioconda conda config --add channels conda-forge","title":"Configuring channels"},{"location":"02_conda/#creating-an-environment","text":"Now that we have conda installed and our channels configured, we are ready to create an environment. conda create -y -n dib_rotation This creates an empty environment named dib_rotation . To activate this environment, run: conda activate dib_rotation Your prompt should now start with (dib_rotation) . We can now install software into our environment. Let's install sourmash, which we will use in a later lesson. conda install -y sourmash","title":"Creating an environment"},{"location":"02_conda/#deactivating-and-exiting","text":"If you would like to leave your environment, you can type conda deactivate and you will return to the base environment. When you log out of farm by typing exit , when you end a tmux or screen session, or when an srun job ends, your environment will automatically be exited. To restart the environment, you can run conda activate dib_rotation .","title":"Deactivating and Exiting"},{"location":"03_background_reading/","text":"Multiple levels of the unknown in microbiome research A good primer on the field of metagenomics Metagenomic Assembly: Overview, Challenges and Applications A good primer on the field of metagenomics Quick insights from sequencing data with sourmash A tutorial and explanation of sourmash, including a definition of k-mers, computing sourmash signatures, comparing RNA-seq samples quickl, detecting eukaryotic contamination in raw RNA-seq reads, building your own database for searching, and other sourmash databases. Large-scale sequence comparisons with sourmash Background on the theory of different commands included in sourmash, accompanied by three small tutorials Exploring neighborhoods in large metagenome assembly graphs using spacegraphcats reveals hidden sequence diversity First spacegraphcats publication that details metagenome-assembled genome bin completion via assembly graph queries. Genome-Resolved Metagenomic Analysis Reveals Roles for Candidate Phyla and Other Microbial Community Members in Biogeochemical Transformations in Oil Reservoirs Describes the dataset used in the spacegraphcats publication. Uses a typical de novo metagenomics workflow. Streamlining Data-Intensive Biology With Workflow Systems A primer on using workflow systems for data-intensive biology, and other tips and tricks for working with large volumes of biological data","title":"Background Reading"},{"location":"04_documentation/","text":"Keeping a Lab Notebook Just like with wetlab work, it's important to document everything you do during a computational workflow. Where to take notes For this project, we recommend using HackMD . HackMD works a little like google docs, but enables better formatting for adding code. You can start with just regular text, but as you start adding screnshots, code blocks, and header sections, you can use Markdown syntax to improve formatting of your rendered notes. HackMD shows you a few examples of Markdown when you first open a new document, but you can also check out this markdown tutorial if you want to learn more. How to take notes It's ok to provide the minimum amount of information necessary to execute a set of commands (i.e., you don't necessary have to record every failure, every ls , etc), but it is important to document each step. + Copying and pasting the commands that worked is a great way to record them (the history command can be helpful to see what you've run in the past). Documentation is for you! (And also for others) Your lab notebook and documentation is most useful for future you. Keep in mind that things that seem super obvious right now will likely be forgetten within a few weeks/months. Try to be detailed enough so that if you tried to pick up this project again in 3 months (or 3 years), you would be able to understand exactly what to do and how to do it. With a good lab notebook, you can save yourself from troubleshooting the same errors over and over again, as well as greatly simplify the process of writing up your Materials and Methods section for any reports or papers. Finally, good lab notebooks help keep everyone working on your project (both now and in the future) on the same page. Other systems for taking notes: After this project, if you like HackMD, great! Stick with it. If not: Using google docs or Microsoft Word for documenting computer commands can be hard because of autocorrection. We generally recommend against using these programs. Using a plain text editor (Notepad, Notepad++, Atom, BBEdit, TextEdit, nano, vim) avoids autocorrect problems but still has a nice user interface. Jupyter Lab is very useful for interactive research explorations and notetaking. We'll try this out in a later section. Eventually, we'll work through using git and GitHub to record and version control our workflows, but for now it's enough to write down everything you do.","title":"Keeping a Lab Notebook"},{"location":"04_documentation/#keeping-a-lab-notebook","text":"Just like with wetlab work, it's important to document everything you do during a computational workflow.","title":"Keeping a Lab Notebook"},{"location":"04_documentation/#where-to-take-notes","text":"For this project, we recommend using HackMD . HackMD works a little like google docs, but enables better formatting for adding code. You can start with just regular text, but as you start adding screnshots, code blocks, and header sections, you can use Markdown syntax to improve formatting of your rendered notes. HackMD shows you a few examples of Markdown when you first open a new document, but you can also check out this markdown tutorial if you want to learn more.","title":"Where to take notes"},{"location":"04_documentation/#how-to-take-notes","text":"It's ok to provide the minimum amount of information necessary to execute a set of commands (i.e., you don't necessary have to record every failure, every ls , etc), but it is important to document each step. + Copying and pasting the commands that worked is a great way to record them (the history command can be helpful to see what you've run in the past).","title":"How to take notes"},{"location":"04_documentation/#documentation-is-for-you-and-also-for-others","text":"Your lab notebook and documentation is most useful for future you. Keep in mind that things that seem super obvious right now will likely be forgetten within a few weeks/months. Try to be detailed enough so that if you tried to pick up this project again in 3 months (or 3 years), you would be able to understand exactly what to do and how to do it. With a good lab notebook, you can save yourself from troubleshooting the same errors over and over again, as well as greatly simplify the process of writing up your Materials and Methods section for any reports or papers. Finally, good lab notebooks help keep everyone working on your project (both now and in the future) on the same page.","title":"Documentation is for you! (And also for others)"},{"location":"04_documentation/#other-systems-for-taking-notes","text":"After this project, if you like HackMD, great! Stick with it. If not: Using google docs or Microsoft Word for documenting computer commands can be hard because of autocorrection. We generally recommend against using these programs. Using a plain text editor (Notepad, Notepad++, Atom, BBEdit, TextEdit, nano, vim) avoids autocorrect problems but still has a nice user interface. Jupyter Lab is very useful for interactive research explorations and notetaking. We'll try this out in a later section. Eventually, we'll work through using git and GitHub to record and version control our workflows, but for now it's enough to write down everything you do.","title":"Other systems for taking notes:"},{"location":"04b_starting_a_work_session/","text":"Starting a Work Session on FARM Any time you log onto FARM to work on this project, follow these steps to get access to computing resources. 1. Log in to FARM If you haven't set up your account yet, start here first. 2. Enter a tmux session When you login to farm, you use your internet connection to create a secure connection between your computer and the cluster. If your internet connection is disrupted, even momentarily, your connection to the cluster as well as any commands that you were running will be interupted and cancelled. For long running commands, this creates a problem. Even if you have great internet connection, you may want to shut your computer or move to a different location. In order to keep a \"session\" alive and to allow a command to continue to run when we close our connection to farm, we use a tool called tmux. You can think of tmux as minimizing your session and allowing things to run. For more on tmux, please see this lesson . This command creates a new tmux session: tmux new -s dib Note: If you already created this session, and want to re-join it, use tmux a -t dib instead. 3. Get access to a compute node When you log on to our FARM computing system, you'll be on a login node, which is basically a computer with very few resources. These login nodes are shared among all users on farm. You can tell you're on the login node (the \"head\" node) because your prompt will start with username@farm . If we run any computing on the login node, logging into and navigating farm will slow down for everyone else! Instead, the moment that we want to do anything substantial, we want to ask farm for a more capable comptuter. Farm uses a \"job scheduler\" to make sure everyone gets access to the computational resources that they need. We can use the following command to create an interactive session with compute resources that will fit our needs: srun -p bmm -J rotation -t 5:00:00 --mem=10G --pty bash srun uses the computer's job scheduler SLURM to allocate you a computer -p specifies the job queue we want to use, and is specific to our farm accounts. -J rotation is the \"job name\" assigned to this session. It can be modified to give your session a more descriptive name, e.g. -J download-data -t denotes that we want the computer for that amount of time (in this case, 3 hours). --mem specifies the amount of memory we'd like the computer to have. Here we've asked for 10 Gigabytes (10G). --pty bash specified that we want the linux shell to be the bash shell, which is the standard shell we've been working wiht so far After running this command, you'll see that your prompt changes from usernames@farm to username@bm1 or some other number. Note that your home directory (the files you see) will be the same for both the login node and the computer you get access to. This is because both read and write from the same hard drives. This means you can create files while in an srun session, and they'll still be there for you when you logout. 4. Activate your Conda Environment Once you're in an srun session, activate your project environment to get access to the software you've installed (or to start installing). conda activate dib_rotation Leaving your tmux session Exit tmux by Ctrl-b , d Reattaching to your tmux session tmux attach Note: if you make more than one tmux session, you can see all session names by typing tmux ls , and then attaching to the right one with tmux attach -t <NAME>","title":"Starting a Work Session"},{"location":"04b_starting_a_work_session/#starting-a-work-session-on-farm","text":"Any time you log onto FARM to work on this project, follow these steps to get access to computing resources.","title":"Starting a Work Session on FARM"},{"location":"04b_starting_a_work_session/#1-log-in-to-farm","text":"If you haven't set up your account yet, start here first.","title":"1. Log in to FARM"},{"location":"04b_starting_a_work_session/#2-enter-a-tmux-session","text":"When you login to farm, you use your internet connection to create a secure connection between your computer and the cluster. If your internet connection is disrupted, even momentarily, your connection to the cluster as well as any commands that you were running will be interupted and cancelled. For long running commands, this creates a problem. Even if you have great internet connection, you may want to shut your computer or move to a different location. In order to keep a \"session\" alive and to allow a command to continue to run when we close our connection to farm, we use a tool called tmux. You can think of tmux as minimizing your session and allowing things to run. For more on tmux, please see this lesson . This command creates a new tmux session: tmux new -s dib Note: If you already created this session, and want to re-join it, use tmux a -t dib instead.","title":"2. Enter a tmux session"},{"location":"04b_starting_a_work_session/#3-get-access-to-a-compute-node","text":"When you log on to our FARM computing system, you'll be on a login node, which is basically a computer with very few resources. These login nodes are shared among all users on farm. You can tell you're on the login node (the \"head\" node) because your prompt will start with username@farm . If we run any computing on the login node, logging into and navigating farm will slow down for everyone else! Instead, the moment that we want to do anything substantial, we want to ask farm for a more capable comptuter. Farm uses a \"job scheduler\" to make sure everyone gets access to the computational resources that they need. We can use the following command to create an interactive session with compute resources that will fit our needs: srun -p bmm -J rotation -t 5:00:00 --mem=10G --pty bash srun uses the computer's job scheduler SLURM to allocate you a computer -p specifies the job queue we want to use, and is specific to our farm accounts. -J rotation is the \"job name\" assigned to this session. It can be modified to give your session a more descriptive name, e.g. -J download-data -t denotes that we want the computer for that amount of time (in this case, 3 hours). --mem specifies the amount of memory we'd like the computer to have. Here we've asked for 10 Gigabytes (10G). --pty bash specified that we want the linux shell to be the bash shell, which is the standard shell we've been working wiht so far After running this command, you'll see that your prompt changes from usernames@farm to username@bm1 or some other number. Note that your home directory (the files you see) will be the same for both the login node and the computer you get access to. This is because both read and write from the same hard drives. This means you can create files while in an srun session, and they'll still be there for you when you logout.","title":"3. Get access to a compute node"},{"location":"04b_starting_a_work_session/#4-activate-your-conda-environment","text":"Once you're in an srun session, activate your project environment to get access to the software you've installed (or to start installing). conda activate dib_rotation","title":"4. Activate your Conda Environment"},{"location":"04b_starting_a_work_session/#leaving-your-tmux-session","text":"Exit tmux by Ctrl-b , d","title":"Leaving your tmux session"},{"location":"04b_starting_a_work_session/#reattaching-to-your-tmux-session","text":"tmux attach Note: if you make more than one tmux session, you can see all session names by typing tmux ls , and then attaching to the right one with tmux attach -t <NAME>","title":"Reattaching to your tmux session"},{"location":"05_starting_with_data/","text":"We will be working with a metagenome sample from an Alaskan Oil Reservoir sequenced in this study . The sample is named SB1 because it was sampled from the Schrader Bluff Formation. We will start working with this sample by downloading it. Then, we will assess its quality using FastQC. Finding Sequencing Data There are many ways to discover and download sequencing data. In this case, we are using a sample that was detailed in a paper. When this is the case, it is usually easiest to find this sample by looking at the paper itself. We can see on Page 9 that the authors have documented the accession numbers of their samples (metagenome-assembled genomes and raw sequences) in the paper: We can use this information to find a link to use to download the raw sequencing data. We see that the raw reads are in GenBank under the accession SRP057267 . Let's navigate to GenBank and see what these files look like there. Searching all databases for SRP057267 , we find the following result: https://www.ncbi.nlm.nih.gov/sra/SRP057267 We see that the raw reads are in the Sequence Read Archive. The last sample is the one we are interested in: metagenome SB1 from not soured petroleum reservoir, Schrader bluffer formation, Alaska North Slope . Clicking on this link, we see the following description: If we click on the run link SRR1976948 , this takes us to the SRA Run Browser. The Run Browser provides some information about the raw reads, including a estimate of taxonomic composition. It's worth it to explore the Run Browser, but there are easier ways to download data than using the SRA. The SRA is mirrored by the European Nucleotide Archive (ENA) . We can search for our accession number on the ENA as well. If we click on the result, we see the following page: Downloading Sequencing Data We can click directly on the FASTQ files (FTP) to download them to our computer. Alternative, we can copy the links and use these to download the files to a remote computer like Farm. Before we download these files, let's set up a directory structure that will help us stay organized. cd ~ mkdir 2020_rotation_project cd 2020_rotation_project mkdir raw_data cd raw_data Then, we can download the data into our raw_data directory. wget ftp://ftp.sra.ebi.ac.uk/vol1/fastq/SRR197/008/SRR1976948/SRR1976948_1.fastq.gz wget ftp://ftp.sra.ebi.ac.uk/vol1/fastq/SRR197/008/SRR1976948/SRR1976948_2.fastq.gz FASTQ format Although it looks complicated (and it is), we can understand the fastq format with a little decoding. Some rules about the format include... Line Description 1 Always begins with '@' and then information about the read 2 The actual DNA sequence 3 Always begins with a '+' and sometimes the same info in line 1 4 Has a string of characters which represent the quality scores; must have same number of characters as line 2 We can view the first complete read in a fastq file by using head to look at the first four lines. Because the our files are gzipped, we first temporarily decompress them with zcat . zcat SRR1976948_2.fastq.gz | head -n 4 Using a different SRR accession for example purposes, the first four lines of the file look something like this: @SRR2584863.1 HWI-ST957:244:H73TDADXX:1:1101:4712:2181/1 TTCACATCCTGACCATTCAGTTGAGCAAAATAGTTCTTCAGTGCCTGTTTAACCGAGTCACGCAGGGGTTTTTGGGTTACCTGATCCTGAGAGTTAACGGTAGAAACGGTCAGTACGTCAGAATTTACGCGTTGTTCGAACATAGTTCTG + CCCFFFFFGHHHHJIJJJJIJJJIIJJJJIIIJJGFIIIJEDDFEGGJIFHHJIJJDECCGGEGIIJFHFFFACD:BBBDDACCCCAA@@CA@C>C3>@5(8&>C:9?8+89<4(:83825C(:A######################### Line 4 shows the quality for each nucleotide in the read. Quality is interpreted as the probability of an incorrect base call (e.g. 1 in 10) or, equivalently, the base call accuracy (e.g. 90%). To make it possible to line up each individual nucleotide with its quality score, the numerical score is converted into a code where each individual character represents the numerical quality score for an individual nucleotide. 'For example, in the line above, the quality score line is: CCCFFFFFGHHHHJIJJJJIJJJIIJJJJIIIJJGFIIIJEDDFEGGJIFHHJIJJDECCGGEGIIJFHFFFACD:BBBDDACCCCAA@@CA@C>C3>@5(8&>C:9?8+89<4(:83825C(:A######################### The numerical value assigned to each of these characters depends on the sequencing platform that generated the reads. The sequencing machine used to generate our data uses the standard Sanger quality PHRED score encoding, using Illumina version 1.8 onwards. Each character is assigned a quality score between 0 and 41 as shown in the chart below. Quality encoding: !\"#$%&'()*+,-./0123456789:;<=>?@ABCDEFGHIJ | | | | | Quality score: 01........11........21........31........41 Each quality score represents the probability that the corresponding nucleotide call is incorrect. This quality score is logarithmically based, so a quality score of 10 reflects a base call accuracy of 90%, but a quality score of 20 reflects a base call accuracy of 99%. These probability values are the results from the base calling algorithm and depend on how much signal was captured for the base incorporation. Looking back at our example read: @SRR2584863.1 HWI-ST957:244:H73TDADXX:1:1101:4712:2181/1 TTCACATCCTGACCATTCAGTTGAGCAAAATAGTTCTTCAGTGCCTGTTTAACCGAGTCACGCAGGGGTTTTTGGGTTACCTGATCCTGAGAGTTAACGGTAGAAACGGTCAGTACGTCAGAATTTACGCGTTGTTCGAACATAGTTCTG + CCCFFFFFGHHHHJIJJJJIJJJIIJJJJIIIJJGFIIIJEDDFEGGJIFHHJIJJDECCGGEGIIJFHFFFACD:BBBDDACCCCAA@@CA@C>C3>@5(8&>C:9?8+89<4(:83825C(:A######################### we can now see that there is a range of quality scores, but that the end of the sequence is very poor ( # = a quality score of 2). How does the first read in SRR1976948_2.fastq.gz compare to this example? Assessing Quality with FastQC In real life, you won't be assessing the quality of your reads by visually inspecting your FASTQ files. Rather, you'll be using a software program to assess read quality and filter out poor quality reads. We'll first use a program called FastQC to visualize the quality of our reads. FastQC has a number of features which can give you a quick impression of any problems your data may have, so you can take these issues into consideration before moving forward with your analyses. Rather than looking at quality scores for each individual read, FastQC looks at quality collectively across all reads within a sample. The image below shows one FastQC-generated plot that indicatesa very high quality sample: The x-axis displays the base position in the read, and the y-axis shows quality scores. In this example, the sample contains reads that are 40 bp long. This is much shorter than the reads we are working with in our workflow. For each position, there is a box-and-whisker plot showing the distribution of quality scores for all reads at that position. The horizontal red line indicates the median quality score and the yellow box shows the 1st to 3rd quartile range. This means that 50% of reads have a quality score that falls within the range of the yellow box at that position. The whiskers show the absolute range, which covers the lowest (0th quartile) to highest (4th quartile) values. For each position in this sample, the quality values do not drop much lower than 32. This is a high quality score. The plot background is also color-coded to identify good (green), acceptable (yellow), and bad (red) quality scores. Now let's take a look at a quality plot on the other end of the spectrum. Here, we see positions within the read in which the boxes span a much wider range. Also, quality scores drop quite low into the \"bad\" range, particularly on the tail end of the reads. The FastQC tool produces several other diagnostic plots to assess sample quality, in addition to the one plotted above. Running FastQC We will now assess the quality of the reads that we downloaded. First, make sure you're still in the raw_data directory cd ~/2020_rotation_project/raw_data Then, use conda to install fastqc. Make sure you activate your rotation environment. conda activate dib_rotation conda install fastqc FastQC can accept multiple file names as input, and on both zipped and unzipped files, so we can use the *.fastq* wildcard to run FastQC on all of the FASTQ files in this directory. fastqc *.fastq* The FastQC program has created several new files within our directory. For each input FASTQ file, FastQC has created a .zip file and a .html file. The .zip file extension indicates that this is actually a compressed set of multiple output files. We'll be working with these output files soon. The .html file is a stable webpage displaying the summary report for each of our samples. Transferring data from Farm to your computer To transfer a file from a remote server to our own machines, we will use scp . To learn more about scp , see the bottom of this tutorial . We've currently been running commands on a terminal open on farm . In order to transfer to our own computer, we want a terminal open on our local computer. There are two ways to do this: open a second terminal tab/window, or close tmux and exit farm. Let's make a new window. Once you've opened a second terminal window, you need to make sure you know what file system each window is pointing to. If you're on farm (the original window), the prompt should say (dib_rotation) USERNAME@<SRUN NODE> . Your new window should just have USERNAME@<YOUR COMPUTER NAME> . There will be no (dib_rotation) unless you've created and activated that conda environment on your local computer as well. Now, from the terminal open to your local computer, copy over the fastqc HTML files using scp , a \"secure copy\" program. scp -P 2022 -i /path/to/key/file username@farm.cse.ucdavis.edu:~/2020_rotation_project/raw_data/*.html . -P 2022 tells scp to use the 2022 port on farm, which is our data transfer port replace /path/to/key/file with the path to your ssh keygen file, created during setup both cp and scp commands use the format: cp <FILE_TO_TRANSFER> <DESTINATION> . This command will copy the files down to the directory that you're sitting in on your home computer. If you're on a windows machine, you may need to copy it from your linux partition over to your windows partition (perhaps most easily accessible in a downloads folder or your desktop). Once the file is on your local computer, double click on it and it will open in your browser. You can now explore the FastQC output. Decoding the FastQC Output We've now looked at quite a few \"Per base sequence quality\" FastQC graphs, but there are nine other graphs that we haven't talked about! Below we have provided a brief overview of interpretations for each of these plots. For more information, please see the FastQC documentation here . Per tile sequence quality : the machines that perform sequencing are divided into tiles. This plot displays patterns in base quality along these tiles. Consistently low scores are often found around the edges, but hot spots can also occur in the middle if an air bubble was introduced at some point during the run. Per sequence quality scores : a density plot of quality for all reads at all positions. This plot shows what quality scores are most common. Per base sequence content : plots the proportion of each base position over all of the reads. Typically, we expect to see each base roughly 25% of the time at each position, but this often fails at the beginning or end of the read due to quality or adapter content. Per sequence GC content : a density plot of average GC content in each of the reads. Per base N content : the percent of times that 'N' occurs at a position in all reads. If there is an increase at a particular position, this might indicate that something went wrong during sequencing. Sequence Length Distribution : the distribution of sequence lengths of all reads in the file. If the data is raw, there is often on sharp peak, however if the reads have been trimmed, there may be a distribution of shorter lengths. Sequence Duplication Levels : A distribution of duplicated sequences. In sequencing, we expect most reads to only occur once. If some sequences are occurring more than once, it might indicate enrichment bias (e.g. from PCR). If the samples are high coverage (or RNA-seq or amplicon), this might not be true. Overrepresented sequences : A list of sequences that occur more frequently than would be expected by chance. Adapter Content : a graph indicating where adapater sequences occur in the reads. K-mer Content : a graph showing any sequences which may show a positional bias within the reads. Extra Info if you ever need to download >10 accessions from the SRA, the sra-toolkit is a great tool to do this with! However, we find sra-toolkit cumbersome when only a couple accessions need to be downloaded.","title":"Find and download the data"},{"location":"05_starting_with_data/#finding-sequencing-data","text":"There are many ways to discover and download sequencing data. In this case, we are using a sample that was detailed in a paper. When this is the case, it is usually easiest to find this sample by looking at the paper itself. We can see on Page 9 that the authors have documented the accession numbers of their samples (metagenome-assembled genomes and raw sequences) in the paper: We can use this information to find a link to use to download the raw sequencing data. We see that the raw reads are in GenBank under the accession SRP057267 . Let's navigate to GenBank and see what these files look like there. Searching all databases for SRP057267 , we find the following result: https://www.ncbi.nlm.nih.gov/sra/SRP057267 We see that the raw reads are in the Sequence Read Archive. The last sample is the one we are interested in: metagenome SB1 from not soured petroleum reservoir, Schrader bluffer formation, Alaska North Slope . Clicking on this link, we see the following description: If we click on the run link SRR1976948 , this takes us to the SRA Run Browser. The Run Browser provides some information about the raw reads, including a estimate of taxonomic composition. It's worth it to explore the Run Browser, but there are easier ways to download data than using the SRA. The SRA is mirrored by the European Nucleotide Archive (ENA) . We can search for our accession number on the ENA as well. If we click on the result, we see the following page:","title":"Finding Sequencing Data"},{"location":"05_starting_with_data/#downloading-sequencing-data","text":"We can click directly on the FASTQ files (FTP) to download them to our computer. Alternative, we can copy the links and use these to download the files to a remote computer like Farm. Before we download these files, let's set up a directory structure that will help us stay organized. cd ~ mkdir 2020_rotation_project cd 2020_rotation_project mkdir raw_data cd raw_data Then, we can download the data into our raw_data directory. wget ftp://ftp.sra.ebi.ac.uk/vol1/fastq/SRR197/008/SRR1976948/SRR1976948_1.fastq.gz wget ftp://ftp.sra.ebi.ac.uk/vol1/fastq/SRR197/008/SRR1976948/SRR1976948_2.fastq.gz","title":"Downloading Sequencing Data"},{"location":"05_starting_with_data/#fastq-format","text":"Although it looks complicated (and it is), we can understand the fastq format with a little decoding. Some rules about the format include... Line Description 1 Always begins with '@' and then information about the read 2 The actual DNA sequence 3 Always begins with a '+' and sometimes the same info in line 1 4 Has a string of characters which represent the quality scores; must have same number of characters as line 2 We can view the first complete read in a fastq file by using head to look at the first four lines. Because the our files are gzipped, we first temporarily decompress them with zcat . zcat SRR1976948_2.fastq.gz | head -n 4 Using a different SRR accession for example purposes, the first four lines of the file look something like this: @SRR2584863.1 HWI-ST957:244:H73TDADXX:1:1101:4712:2181/1 TTCACATCCTGACCATTCAGTTGAGCAAAATAGTTCTTCAGTGCCTGTTTAACCGAGTCACGCAGGGGTTTTTGGGTTACCTGATCCTGAGAGTTAACGGTAGAAACGGTCAGTACGTCAGAATTTACGCGTTGTTCGAACATAGTTCTG + CCCFFFFFGHHHHJIJJJJIJJJIIJJJJIIIJJGFIIIJEDDFEGGJIFHHJIJJDECCGGEGIIJFHFFFACD:BBBDDACCCCAA@@CA@C>C3>@5(8&>C:9?8+89<4(:83825C(:A######################### Line 4 shows the quality for each nucleotide in the read. Quality is interpreted as the probability of an incorrect base call (e.g. 1 in 10) or, equivalently, the base call accuracy (e.g. 90%). To make it possible to line up each individual nucleotide with its quality score, the numerical score is converted into a code where each individual character represents the numerical quality score for an individual nucleotide. 'For example, in the line above, the quality score line is: CCCFFFFFGHHHHJIJJJJIJJJIIJJJJIIIJJGFIIIJEDDFEGGJIFHHJIJJDECCGGEGIIJFHFFFACD:BBBDDACCCCAA@@CA@C>C3>@5(8&>C:9?8+89<4(:83825C(:A######################### The numerical value assigned to each of these characters depends on the sequencing platform that generated the reads. The sequencing machine used to generate our data uses the standard Sanger quality PHRED score encoding, using Illumina version 1.8 onwards. Each character is assigned a quality score between 0 and 41 as shown in the chart below. Quality encoding: !\"#$%&'()*+,-./0123456789:;<=>?@ABCDEFGHIJ | | | | | Quality score: 01........11........21........31........41 Each quality score represents the probability that the corresponding nucleotide call is incorrect. This quality score is logarithmically based, so a quality score of 10 reflects a base call accuracy of 90%, but a quality score of 20 reflects a base call accuracy of 99%. These probability values are the results from the base calling algorithm and depend on how much signal was captured for the base incorporation. Looking back at our example read: @SRR2584863.1 HWI-ST957:244:H73TDADXX:1:1101:4712:2181/1 TTCACATCCTGACCATTCAGTTGAGCAAAATAGTTCTTCAGTGCCTGTTTAACCGAGTCACGCAGGGGTTTTTGGGTTACCTGATCCTGAGAGTTAACGGTAGAAACGGTCAGTACGTCAGAATTTACGCGTTGTTCGAACATAGTTCTG + CCCFFFFFGHHHHJIJJJJIJJJIIJJJJIIIJJGFIIIJEDDFEGGJIFHHJIJJDECCGGEGIIJFHFFFACD:BBBDDACCCCAA@@CA@C>C3>@5(8&>C:9?8+89<4(:83825C(:A######################### we can now see that there is a range of quality scores, but that the end of the sequence is very poor ( # = a quality score of 2). How does the first read in SRR1976948_2.fastq.gz compare to this example?","title":"FASTQ format"},{"location":"05_starting_with_data/#assessing-quality-with-fastqc","text":"In real life, you won't be assessing the quality of your reads by visually inspecting your FASTQ files. Rather, you'll be using a software program to assess read quality and filter out poor quality reads. We'll first use a program called FastQC to visualize the quality of our reads. FastQC has a number of features which can give you a quick impression of any problems your data may have, so you can take these issues into consideration before moving forward with your analyses. Rather than looking at quality scores for each individual read, FastQC looks at quality collectively across all reads within a sample. The image below shows one FastQC-generated plot that indicatesa very high quality sample: The x-axis displays the base position in the read, and the y-axis shows quality scores. In this example, the sample contains reads that are 40 bp long. This is much shorter than the reads we are working with in our workflow. For each position, there is a box-and-whisker plot showing the distribution of quality scores for all reads at that position. The horizontal red line indicates the median quality score and the yellow box shows the 1st to 3rd quartile range. This means that 50% of reads have a quality score that falls within the range of the yellow box at that position. The whiskers show the absolute range, which covers the lowest (0th quartile) to highest (4th quartile) values. For each position in this sample, the quality values do not drop much lower than 32. This is a high quality score. The plot background is also color-coded to identify good (green), acceptable (yellow), and bad (red) quality scores. Now let's take a look at a quality plot on the other end of the spectrum. Here, we see positions within the read in which the boxes span a much wider range. Also, quality scores drop quite low into the \"bad\" range, particularly on the tail end of the reads. The FastQC tool produces several other diagnostic plots to assess sample quality, in addition to the one plotted above.","title":"Assessing Quality with FastQC"},{"location":"05_starting_with_data/#running-fastqc","text":"We will now assess the quality of the reads that we downloaded. First, make sure you're still in the raw_data directory cd ~/2020_rotation_project/raw_data Then, use conda to install fastqc. Make sure you activate your rotation environment. conda activate dib_rotation conda install fastqc FastQC can accept multiple file names as input, and on both zipped and unzipped files, so we can use the *.fastq* wildcard to run FastQC on all of the FASTQ files in this directory. fastqc *.fastq* The FastQC program has created several new files within our directory. For each input FASTQ file, FastQC has created a .zip file and a .html file. The .zip file extension indicates that this is actually a compressed set of multiple output files. We'll be working with these output files soon. The .html file is a stable webpage displaying the summary report for each of our samples.","title":"Running FastQC"},{"location":"05_starting_with_data/#transferring-data-from-farm-to-your-computer","text":"To transfer a file from a remote server to our own machines, we will use scp . To learn more about scp , see the bottom of this tutorial . We've currently been running commands on a terminal open on farm . In order to transfer to our own computer, we want a terminal open on our local computer. There are two ways to do this: open a second terminal tab/window, or close tmux and exit farm. Let's make a new window. Once you've opened a second terminal window, you need to make sure you know what file system each window is pointing to. If you're on farm (the original window), the prompt should say (dib_rotation) USERNAME@<SRUN NODE> . Your new window should just have USERNAME@<YOUR COMPUTER NAME> . There will be no (dib_rotation) unless you've created and activated that conda environment on your local computer as well. Now, from the terminal open to your local computer, copy over the fastqc HTML files using scp , a \"secure copy\" program. scp -P 2022 -i /path/to/key/file username@farm.cse.ucdavis.edu:~/2020_rotation_project/raw_data/*.html . -P 2022 tells scp to use the 2022 port on farm, which is our data transfer port replace /path/to/key/file with the path to your ssh keygen file, created during setup both cp and scp commands use the format: cp <FILE_TO_TRANSFER> <DESTINATION> . This command will copy the files down to the directory that you're sitting in on your home computer. If you're on a windows machine, you may need to copy it from your linux partition over to your windows partition (perhaps most easily accessible in a downloads folder or your desktop). Once the file is on your local computer, double click on it and it will open in your browser. You can now explore the FastQC output.","title":"Transferring data from Farm to your computer"},{"location":"05_starting_with_data/#decoding-the-fastqc-output","text":"We've now looked at quite a few \"Per base sequence quality\" FastQC graphs, but there are nine other graphs that we haven't talked about! Below we have provided a brief overview of interpretations for each of these plots. For more information, please see the FastQC documentation here . Per tile sequence quality : the machines that perform sequencing are divided into tiles. This plot displays patterns in base quality along these tiles. Consistently low scores are often found around the edges, but hot spots can also occur in the middle if an air bubble was introduced at some point during the run. Per sequence quality scores : a density plot of quality for all reads at all positions. This plot shows what quality scores are most common. Per base sequence content : plots the proportion of each base position over all of the reads. Typically, we expect to see each base roughly 25% of the time at each position, but this often fails at the beginning or end of the read due to quality or adapter content. Per sequence GC content : a density plot of average GC content in each of the reads. Per base N content : the percent of times that 'N' occurs at a position in all reads. If there is an increase at a particular position, this might indicate that something went wrong during sequencing. Sequence Length Distribution : the distribution of sequence lengths of all reads in the file. If the data is raw, there is often on sharp peak, however if the reads have been trimmed, there may be a distribution of shorter lengths. Sequence Duplication Levels : A distribution of duplicated sequences. In sequencing, we expect most reads to only occur once. If some sequences are occurring more than once, it might indicate enrichment bias (e.g. from PCR). If the samples are high coverage (or RNA-seq or amplicon), this might not be true. Overrepresented sequences : A list of sequences that occur more frequently than would be expected by chance. Adapter Content : a graph indicating where adapater sequences occur in the reads. K-mer Content : a graph showing any sequences which may show a positional bias within the reads.","title":"Decoding the FastQC Output"},{"location":"05_starting_with_data/#extra-info","text":"if you ever need to download >10 accessions from the SRA, the sra-toolkit is a great tool to do this with! However, we find sra-toolkit cumbersome when only a couple accessions need to be downloaded.","title":"Extra Info"},{"location":"06_quality_control/","text":"Quality Control the Data If you're starting a new work session on FARM, be sure to follow the instructions here . After downloading sequencing data , the next step in many pipelines is to perform quality control trimming on the reads. However, deciding when and how to trim data is pipeline dependent. Below, we define a few types of quality control and explore a use cases and how trimming recommendations may change with different applications. Although this project focuses on metagenomic sequencing, we include other applications in this discussion. Types of Quality Control Adapter and barcode trimming : Adapter sequences are added to a sample library to aid in the physical process of sequencing. They are ubiquitous within a certain chemistry, and so are present across all sequenced samples. Barcodes are unique nucleotide sequences used to identify a specific sample when multiple samples are sequenced in a single lane. After barcoded samples are separated from one another in a process called demultiplexing, barcodes are no longer needed in a sequence. It is generally a good idea to remove adapters and barcodes from sequencing samples before proceeding with any downstream application. However, if you are using a pipeline that involves matching between reads and a quality reference, you may get similar results with or without adapter trimming. For quick estimation Quality trimming : Quality trimming removes low-quality bases from sequences reads. The user can set the stringency cut off for \"low quality\" by indicating a phred score at which to trim. K-mer trimming : K-mer trimming removes k-mers that occur very few times in a sequencing dataset. In reads with sufficient sequencing depth, we expect real k-mers to occur multiple times. When a single sequencing error occurs in a read, this produces k erroneous k-mers. K-mer trimming trims a read to remove all of these k-mers. K-mer trimming does not rely on information from the sequencer like phred scores, but instead on the biological signal in the reads themselves. When and how to trim? Trimming is a balance of removing artificial or incorrect nucleotides and retaining true nucleotides in sequencing data. What and when to trim therefore changes with the sequencing application, and with the sequencing data itself. Below we explore some trimming use cases to help develop an intuition for what type of trimming is necessary and when. Single-species genomic sequencing for assembly : Let's imagine we have just sequenced an Escherichia coli isolate with 100X coverage and would like to assemble the isolate. We would first want to remove adapters and barcodes to prevent these sequences from ending up in our final assembly. Then, stringent quality and k-mer trimming may be appropriate, because we have high coverage data; even if we were to stringently trim and were only left with 50% of our original number of reads, we would still have 50X coverage of very high quality data. 50X coverage is sufficient to acheive a good bacterial assembly in most cases. de novo RNA-sequencing assembly Now let's imagine we have sequenced the transcriptome of our favorite species which does not currently have a reference transcriptome. Because RNA transcripts have different abundance profiles, we can't use average coverage in the same way as we used it for single-species genomic sequencing. We need to be more careful when we k-mer and error trim so as not to accidentally remove low-abundance reads that represent true transcripts. We would likely use light quality trimming (e.g. a phred score of ~5). For k-mer trimming, we would only trim reads that contain high-abundance k-mers. Metagenome de novo assembly Trimming metagenomic reads for de novo assembly is similar to trimming RNA-sequencing reads for de novo transcriptome assembly. Because there are often low-abundance organisms that have low-coverage in our sequencing datasets, we need to be careful not to accidently remove these during trimming. Metagenome read mapping In referenced-based analyses including mapping of metagenomic reads to a set of reference genomes, reads will often map even when they contain adapters and barcodes. However, in some cases, the presence of adapters and barcodes does prevent mapping, so it is safer to remove all barcodes and adapters. References about trimming Many scientific studies have explored the trimming parameter space in an effort to make recommendations for different applications. We include some of these studies below. On the optimal trimming of high-throughput mRNA sequence data An Extensive Evaluation of Read Trimming Effects on Illumina NGS Data Analysis Quality and Adapter trimming with Fastp We will use fastp to do quality trimming of our reads. In the Downloading Data Module , we saw using FastQC that the Illumina Universal Adapter was present in our samples. We also saw that the sequence read quality dropped dramatically toward the end of the read. We will remove both of these sequences using fastp. Fastp also creates its own FastQC-style html reports for the files that we can look at after running. Run fastp Reminder, make sure you've followed the Starting a Work Session steps to get your Farm session set up. You should be within your dib_rotation conda environment. Install fastp: conda install -y fastp We can now trim our data! Let's set up our directory structure: cd ~/2020_rotation_project mkdir -p trim cd trim Run fastp on the SRR1976948 sample with the following command: fastp --in1 ../raw_data/SRR1976948_1.fastq.gz \\ --in2 ../raw_data/SRR1976948_2.fastq.gz \\ --out1 SRR1976948_1.trim.fastq.gz \\ --out2 SRR1976948_2.trim.fastq.gz \\ --detect_adapter_for_pe \\ --qualified_quality_phred 4 \\ --length_required 31 --correction \\ --json SRR1976948.trim.json \\ --html SRR1976948.trim.html Command Breakdown --in1 , --in2 - the read1 and read2 input file names --out1 , --out2 - the read1 and read2 output file names --detect_adapter_for_pe - Auto detect the adapters for our paired end (PE) reads, and remove them during trimming --length_required - discard reads shorter than length_required paramter (default is 15) --correction - enable base correction if the paired end reads overlap (only for PE data), --qualified_quality_phred - the quality value that a base is qualified. Default 15 means phred quality >=Q15 is qualified. (int [=15]) --html , --json - file name for the fastp trimming report printed to html and/or json format We change the Phred quality score cutoff to 4 to be more lenient in our trimming. Recall from our FastQC lesson that a quality score of 10 indicates a 1 in 10 chance that the base is inaccurate. A score of 20 is a 1 in 100 chance that the base is inaccurate. 30 is 1 in 1,000. And 40 in 1 in 10,000. By using a score of 4, we are more likely to keep data that has a high probability of being accurate. This is especially important in a metagenomics context where using stringent trimming may discard reads from organisms that are low abundance. As done in downloading sequencing data , you can use scp to copy the html report to your computer. Make sure you're running this command from your own computer , not from farm . scp -P 2022 -i /path/to/key/file username@farm.cse.ucdavis.edu:~/2020_rotation_project/trim/*.html ./ If you're on a mac using zsh , you may need to replace the scp with noglob scp in the command above. If you're on windows, you may need to move the the files from the download location on your Linux shell over to the windows side of your computer before opening. Once the file is on your local computer, double click on it and it will open in your browser. You can now explore the fastp trimming report. Why (or why not) do k-mer trimming? Even after quality trimming with fastp, our reads will still contain errors. Why? First, fastp trims based solely on the quality score, which is a statistical statement about the correctness of a base - a Q score of 30 means that, of 1000 bases with that Q score, 1 of those bases will be wrong. So, a base can have a high Q score and still be wrong (and many bases will have a low Q score and still be correct)! Second, we trimmed very lightly - only bases that had a very low quality were removed. This was intentional because we want to retain as much coverage as possible for our downstream techniques (many of which do not suffer too much if some errors remain). An alternative to trimming based on the quality scores is to trim based on k-mer abundance - this is known as k-mer spectral error trimming. K-mer spectral error trimming always beats quality score trimming in terms of eliminating errors; e.g. look at this table from Zhang et al., 2014 : The basic logic is this: if you see low abundance k-mers in a high coverage data set, those k-mers are almost certainly the result of errors. (Caveat: strain variation could also create them.) In metatranscriptomic data sets we do have the problem that we may have very low and very high coverage data. So we don\u2019t necessarily want to get rid of all low-abundance k-mers, because they may represent truly low abundance (but useful) data. As part of the khmer project in our lab, we have developed an approach that sorts reads into high abundance and low abundance reads, and only error trims the high abundance reads. This does mean that many errors may get left in the data set, because we have no way of figuring out if they are errors or simply low coverage, but that\u2019s OK (and you can always trim them off if you really care). Kmer trimming with khmer Next, let's k-mer trim our data. This will take 20GB of RAM and a few hours to complete. We didn't ask for quite that much RAM when we initially got our computer, so we'll need a different one. First, exit your current srun session exit Next, use this srun command to get a larger computer that can handle the k-mer trimming analysis: srun -p bmh -J khmer -t 20:00:00 --mem=21gb -c 1 --pty bash Since we changed computers, our conda environment was automatically deactivated. Activate your project environment again: conda activate dib_rotation Install khmer We need to install the software we will use to perform k-mer trimming, khmer . Make sure you've activated the conda environment you are using for this project before running this command! conda install -y khmer Using khmer for k-mer trimming Once khmer is installed, we can use it for k-mer trimming. Let's get our files and directories set up: cd ~/2020_rotation_project mkdir -p abundtrim cd abundtrim Now we can run k-mer trimming! The first line of this command interleaves our paired end reads, putting them in one file where forward and reverse reads alternate on each line. The second line of this command performs the k-mer trimming. Note that these commands are connected by the pipe ( | ) character. This character means that the first half of the command (before the | ) is executed first, and the output is passed (\"piped\") to the second half of the command (after the | ). interleave-reads.py ../trim/SRR1976948_1.trim.fastq.gz ../trim/SRR1976948_2.trim.fastq.gz | \\ trim-low-abund.py --gzip -C 3 -Z 18 -M 20e9 -V - -o SRR1976948.abundtrim.fq.gz Note: Here, we are referencing the trimmed files using a relative path: ../trim/ . That is, to access these files, we go up one directory level ( .. ), then descend into the trim folder. Assess changes in kmer abundance To see how many k-mers we removed, you can examine the distribution as above, or use the unique-kmers.py script. Let's compare kmers for one sample. unique-kmers.py ../trim/SRR1976948_1.trim.fastq.gz ../trim/SRR1976948_2.trim.fastq.gz unique-kmers.py SRR1976948.abundtrim.fq.gz Note, here we are using a relative path, ../trim/ . That is, to access the SRR1976948_*.trim.fastq.gz files, we go up one directory ( ../ ), then down into trim . The raw adapter-trimmed inputs have an estimated 442441435 unique 32-mers. Estimated number of unique 32-mers in ../trim/SRR1976948_1.trim.fastq.gz: 271670733 Estimated number of unique 32-mers in ../trim/SRR1976948_2.trim.fastq.gz: 370246495 Total estimated number of unique 32-mers: 442441435 The k-mer trimmed file (kmer output) has an estimated unique 32-mers. Estimated number of unique 32-mers in SRR1976948.kmertrim.fq.gz: 329577970 Total estimated number of unique 32-mers: 329577970 Note that the second number is smaller than the first, with over 112m k-mers removed.","title":"Quality Control"},{"location":"06_quality_control/#quality-control-the-data","text":"If you're starting a new work session on FARM, be sure to follow the instructions here . After downloading sequencing data , the next step in many pipelines is to perform quality control trimming on the reads. However, deciding when and how to trim data is pipeline dependent. Below, we define a few types of quality control and explore a use cases and how trimming recommendations may change with different applications. Although this project focuses on metagenomic sequencing, we include other applications in this discussion.","title":"Quality Control the Data"},{"location":"06_quality_control/#types-of-quality-control","text":"Adapter and barcode trimming : Adapter sequences are added to a sample library to aid in the physical process of sequencing. They are ubiquitous within a certain chemistry, and so are present across all sequenced samples. Barcodes are unique nucleotide sequences used to identify a specific sample when multiple samples are sequenced in a single lane. After barcoded samples are separated from one another in a process called demultiplexing, barcodes are no longer needed in a sequence. It is generally a good idea to remove adapters and barcodes from sequencing samples before proceeding with any downstream application. However, if you are using a pipeline that involves matching between reads and a quality reference, you may get similar results with or without adapter trimming. For quick estimation Quality trimming : Quality trimming removes low-quality bases from sequences reads. The user can set the stringency cut off for \"low quality\" by indicating a phred score at which to trim. K-mer trimming : K-mer trimming removes k-mers that occur very few times in a sequencing dataset. In reads with sufficient sequencing depth, we expect real k-mers to occur multiple times. When a single sequencing error occurs in a read, this produces k erroneous k-mers. K-mer trimming trims a read to remove all of these k-mers. K-mer trimming does not rely on information from the sequencer like phred scores, but instead on the biological signal in the reads themselves.","title":"Types of Quality Control"},{"location":"06_quality_control/#when-and-how-to-trim","text":"Trimming is a balance of removing artificial or incorrect nucleotides and retaining true nucleotides in sequencing data. What and when to trim therefore changes with the sequencing application, and with the sequencing data itself. Below we explore some trimming use cases to help develop an intuition for what type of trimming is necessary and when. Single-species genomic sequencing for assembly : Let's imagine we have just sequenced an Escherichia coli isolate with 100X coverage and would like to assemble the isolate. We would first want to remove adapters and barcodes to prevent these sequences from ending up in our final assembly. Then, stringent quality and k-mer trimming may be appropriate, because we have high coverage data; even if we were to stringently trim and were only left with 50% of our original number of reads, we would still have 50X coverage of very high quality data. 50X coverage is sufficient to acheive a good bacterial assembly in most cases. de novo RNA-sequencing assembly Now let's imagine we have sequenced the transcriptome of our favorite species which does not currently have a reference transcriptome. Because RNA transcripts have different abundance profiles, we can't use average coverage in the same way as we used it for single-species genomic sequencing. We need to be more careful when we k-mer and error trim so as not to accidentally remove low-abundance reads that represent true transcripts. We would likely use light quality trimming (e.g. a phred score of ~5). For k-mer trimming, we would only trim reads that contain high-abundance k-mers. Metagenome de novo assembly Trimming metagenomic reads for de novo assembly is similar to trimming RNA-sequencing reads for de novo transcriptome assembly. Because there are often low-abundance organisms that have low-coverage in our sequencing datasets, we need to be careful not to accidently remove these during trimming. Metagenome read mapping In referenced-based analyses including mapping of metagenomic reads to a set of reference genomes, reads will often map even when they contain adapters and barcodes. However, in some cases, the presence of adapters and barcodes does prevent mapping, so it is safer to remove all barcodes and adapters.","title":"When and how to trim?"},{"location":"06_quality_control/#references-about-trimming","text":"Many scientific studies have explored the trimming parameter space in an effort to make recommendations for different applications. We include some of these studies below. On the optimal trimming of high-throughput mRNA sequence data An Extensive Evaluation of Read Trimming Effects on Illumina NGS Data Analysis","title":"References about trimming"},{"location":"06_quality_control/#quality-and-adapter-trimming-with-fastp","text":"We will use fastp to do quality trimming of our reads. In the Downloading Data Module , we saw using FastQC that the Illumina Universal Adapter was present in our samples. We also saw that the sequence read quality dropped dramatically toward the end of the read. We will remove both of these sequences using fastp. Fastp also creates its own FastQC-style html reports for the files that we can look at after running.","title":"Quality and Adapter trimming with Fastp"},{"location":"06_quality_control/#run-fastp","text":"Reminder, make sure you've followed the Starting a Work Session steps to get your Farm session set up. You should be within your dib_rotation conda environment. Install fastp: conda install -y fastp We can now trim our data! Let's set up our directory structure: cd ~/2020_rotation_project mkdir -p trim cd trim Run fastp on the SRR1976948 sample with the following command: fastp --in1 ../raw_data/SRR1976948_1.fastq.gz \\ --in2 ../raw_data/SRR1976948_2.fastq.gz \\ --out1 SRR1976948_1.trim.fastq.gz \\ --out2 SRR1976948_2.trim.fastq.gz \\ --detect_adapter_for_pe \\ --qualified_quality_phred 4 \\ --length_required 31 --correction \\ --json SRR1976948.trim.json \\ --html SRR1976948.trim.html Command Breakdown --in1 , --in2 - the read1 and read2 input file names --out1 , --out2 - the read1 and read2 output file names --detect_adapter_for_pe - Auto detect the adapters for our paired end (PE) reads, and remove them during trimming --length_required - discard reads shorter than length_required paramter (default is 15) --correction - enable base correction if the paired end reads overlap (only for PE data), --qualified_quality_phred - the quality value that a base is qualified. Default 15 means phred quality >=Q15 is qualified. (int [=15]) --html , --json - file name for the fastp trimming report printed to html and/or json format We change the Phred quality score cutoff to 4 to be more lenient in our trimming. Recall from our FastQC lesson that a quality score of 10 indicates a 1 in 10 chance that the base is inaccurate. A score of 20 is a 1 in 100 chance that the base is inaccurate. 30 is 1 in 1,000. And 40 in 1 in 10,000. By using a score of 4, we are more likely to keep data that has a high probability of being accurate. This is especially important in a metagenomics context where using stringent trimming may discard reads from organisms that are low abundance. As done in downloading sequencing data , you can use scp to copy the html report to your computer. Make sure you're running this command from your own computer , not from farm . scp -P 2022 -i /path/to/key/file username@farm.cse.ucdavis.edu:~/2020_rotation_project/trim/*.html ./ If you're on a mac using zsh , you may need to replace the scp with noglob scp in the command above. If you're on windows, you may need to move the the files from the download location on your Linux shell over to the windows side of your computer before opening. Once the file is on your local computer, double click on it and it will open in your browser. You can now explore the fastp trimming report.","title":"Run fastp"},{"location":"06_quality_control/#why-or-why-not-do-k-mer-trimming","text":"Even after quality trimming with fastp, our reads will still contain errors. Why? First, fastp trims based solely on the quality score, which is a statistical statement about the correctness of a base - a Q score of 30 means that, of 1000 bases with that Q score, 1 of those bases will be wrong. So, a base can have a high Q score and still be wrong (and many bases will have a low Q score and still be correct)! Second, we trimmed very lightly - only bases that had a very low quality were removed. This was intentional because we want to retain as much coverage as possible for our downstream techniques (many of which do not suffer too much if some errors remain). An alternative to trimming based on the quality scores is to trim based on k-mer abundance - this is known as k-mer spectral error trimming. K-mer spectral error trimming always beats quality score trimming in terms of eliminating errors; e.g. look at this table from Zhang et al., 2014 : The basic logic is this: if you see low abundance k-mers in a high coverage data set, those k-mers are almost certainly the result of errors. (Caveat: strain variation could also create them.) In metatranscriptomic data sets we do have the problem that we may have very low and very high coverage data. So we don\u2019t necessarily want to get rid of all low-abundance k-mers, because they may represent truly low abundance (but useful) data. As part of the khmer project in our lab, we have developed an approach that sorts reads into high abundance and low abundance reads, and only error trims the high abundance reads. This does mean that many errors may get left in the data set, because we have no way of figuring out if they are errors or simply low coverage, but that\u2019s OK (and you can always trim them off if you really care).","title":"Why (or why not) do k-mer trimming?"},{"location":"06_quality_control/#kmer-trimming-with-khmer","text":"Next, let's k-mer trim our data. This will take 20GB of RAM and a few hours to complete. We didn't ask for quite that much RAM when we initially got our computer, so we'll need a different one. First, exit your current srun session exit Next, use this srun command to get a larger computer that can handle the k-mer trimming analysis: srun -p bmh -J khmer -t 20:00:00 --mem=21gb -c 1 --pty bash Since we changed computers, our conda environment was automatically deactivated. Activate your project environment again: conda activate dib_rotation","title":"Kmer trimming with khmer"},{"location":"06_quality_control/#install-khmer","text":"We need to install the software we will use to perform k-mer trimming, khmer . Make sure you've activated the conda environment you are using for this project before running this command! conda install -y khmer","title":"Install khmer"},{"location":"06_quality_control/#using-khmer-for-k-mer-trimming","text":"Once khmer is installed, we can use it for k-mer trimming. Let's get our files and directories set up: cd ~/2020_rotation_project mkdir -p abundtrim cd abundtrim Now we can run k-mer trimming! The first line of this command interleaves our paired end reads, putting them in one file where forward and reverse reads alternate on each line. The second line of this command performs the k-mer trimming. Note that these commands are connected by the pipe ( | ) character. This character means that the first half of the command (before the | ) is executed first, and the output is passed (\"piped\") to the second half of the command (after the | ). interleave-reads.py ../trim/SRR1976948_1.trim.fastq.gz ../trim/SRR1976948_2.trim.fastq.gz | \\ trim-low-abund.py --gzip -C 3 -Z 18 -M 20e9 -V - -o SRR1976948.abundtrim.fq.gz Note: Here, we are referencing the trimmed files using a relative path: ../trim/ . That is, to access these files, we go up one directory level ( .. ), then descend into the trim folder.","title":"Using khmer for k-mer trimming"},{"location":"06_quality_control/#assess-changes-in-kmer-abundance","text":"To see how many k-mers we removed, you can examine the distribution as above, or use the unique-kmers.py script. Let's compare kmers for one sample. unique-kmers.py ../trim/SRR1976948_1.trim.fastq.gz ../trim/SRR1976948_2.trim.fastq.gz unique-kmers.py SRR1976948.abundtrim.fq.gz Note, here we are using a relative path, ../trim/ . That is, to access the SRR1976948_*.trim.fastq.gz files, we go up one directory ( ../ ), then down into trim . The raw adapter-trimmed inputs have an estimated 442441435 unique 32-mers. Estimated number of unique 32-mers in ../trim/SRR1976948_1.trim.fastq.gz: 271670733 Estimated number of unique 32-mers in ../trim/SRR1976948_2.trim.fastq.gz: 370246495 Total estimated number of unique 32-mers: 442441435 The k-mer trimmed file (kmer output) has an estimated unique 32-mers. Estimated number of unique 32-mers in SRR1976948.kmertrim.fq.gz: 329577970 Total estimated number of unique 32-mers: 329577970 Note that the second number is smaller than the first, with over 112m k-mers removed.","title":"Assess changes in kmer abundance"},{"location":"07_taxonomic_discovery_with_sourmash/","text":"Until now, we've performed general pre-processing steps on our sequencing data; sequence quality analysis and trimming usually occur at the start of any sequencing data analysis pipeline. Now we will begin performing analysis that makes sense for metagenomic sequencing data. We are working with a sample from an Alaskan Oil Reservoir named SB1. We know from reading the Hu et al. paper that this sample contains bacteria and archaea. However, let's pretend that this is a brand new sample that we just got back from our sequencing core. One of the first things we often want to do with new metagenome sequencing samples is figure out their approximate species composition. This allows us to tap in to all of the information known about these species and relate our community to existing literature. We can determine the approximate composition of our sample using sourmash . Introduction to sourmash Please read this tutorial for an introduction to how sourmash works. tl;dr (but actually please read it): sourmash breaks nucleotide sequences down into small pieces, and then searches for those small pieces in databases. This makes it really fast to make comparisons. Here, we will compare our metagenome sample against a pre-prepared database that contains all microbial sequences in GenBank Starting with sourmash Sourmash doesn't have a big memory or CPU footprint, and can be run on most laptops. Below is a recommended srun command to start an interactive session in which to run the srun commands. srun -p bmh -J sourmash24 -t 24:00:00 --mem=16gb -c 1 --pty bash Let's install sourmash conda activate dib_rotation conda install -y -c conda-forge -c bioconda sourmash Next, let's create a directory in which to store our sourmash results cd ~/2020_rotation_project mkdir -p sourmash cd sourmash Then we can link in our raw data. We could run sourmash with our adapter trimmed or k-mer trimmed data. In fact, doing so would make sourmash faster because there would be fewer k-mers in the sample. We are comparing our sample against a database of trusted DNA sequences. If we have adapters or errors in our reads, these won't match to the assemblies in the database. However, even though we very lightly trimmed our reads, there is a chance that we removed a very low abundance organism that was truly present in the sample. ln -s ~/2020_rotation_project/raw_data/*fastq.gz . We need to generate a signature using sourmash. A signature is a compressed representation of the k-mers in the sequence. Using this data structure instead of the reads makes sourmash much faster. sourmash compute -o SRR1976948.sig --merge SRR1976948 --scaled 2000 -k 21,31,51 --track-abundance SRR1976948_*fastq.gz The outputs file, SRR1976948.sig holds a representative subset of k-mers from our original sample, as well as their abundance information. The k-mers are \"hashed\", or transformed, into numbers to make selecting, storing, and looking up the k-mers more efficient. Sourmash lca gather Sourmash provides two methods for estimating the taxonomic composition of known sequences in a metagenome, sourmash gather and sourmash lca gather . sourmash gather gives strain-level specificity to matches in its output -- e.g. all strains that match any sequences (above a threshold) in your metagenome will be reported, along with the percent of each strain that matches. This is useful both to estimate the amount of your metagenome that is known, and to estimate the closest strain relative to the thing that is in your metagenome. sourmash lca gather provides a similar output, but also provides taxonomic information for the match. This means that both an assembly and a taxonomy are required for inclusion in the database. Because of the underlying data structures, sourmash lca gather is faster, but also requires more memory. Farm has plenty of resources, so we will run the faster command. Download and unzip the database: curl -L https://osf.io/4f8n3/download -o genbank-k31.lca.json.gz gunzip genbank-k31.lca.json.gz And then run lca gather sourmash lca gather -o SRR1976948_lca_gather.csv SRR1976948.sig genbank-k31.lca.json We see an output that looks like this: == This is sourmash version 3.0.1. == == Please cite Brown and Irber (2016), doi:10.21105/joss.00027. == loaded 1 LCA databases. ksize=31, scaled=10000 selecting specified query k=31 loaded query: SRR1976948... (k=31) overlap p_query p_match --------- ------- -------- 2.5 Mbp 0.2% 96.9% unassigned Methanomicrobiales archaeon 53_19 2.4 Mbp 0.2% 99.2% Methanobacterium sp. 42_16 2.3 Mbp 0.2% 100.0% Desulfotomaculum sp. 46_80 2.3 Mbp 0.2% 100.0% unassigned Actinobacteria bacterium 66_15 2.1 Mbp 0.2% 97.7% Desulfotomaculum sp. 46_296 2.1 Mbp 0.2% 99.0% Methanosaeta harundinacea 2.0 Mbp 0.2% 99.0% unassigned Marinimicrobia bacterium 46_43 1.9 Mbp 0.2% 100.0% unassigned Bacteroidetes bacterium 38_7 1.9 Mbp 0.2% 55.1% unassigned Thermotogales bacterium EBM-48 The first column estimates the amount of sequences in our metagenome that are contained in the match, while the second column estimates the amount of the match that is contained within our metagenome. These percentages are quite high in the p_match column...that's because the authors who originally analyzed this sample deposited metagenome-assembled genomes from this sample into GenBank. When sourmash is finished running, it tells us that 94% of our sequence was unclassified; i.e. it doesn't match any sequence in the database. This is common for metagenomics, particularly for samples that are sequenced from rare environments (like Alaskan oil reservoirs). In the next lesson, we will work to improve the percent of sequence in the metagenome that is classifiable. Final Thoughts There are many tools like Kraken and Kaiju that can do taxonomic classification of individual reads from metagenomes; these seem to perform well (albeit with high false positive rates) in situations where you don\u2019t necessarily have the genome sequences that are in the metagenome. Sourmash, by contrast, can estimate which known genomes are actually present, so that you can extract them and map/align to them. It seems to have a very low false positive rate and is quite sensitive to strains. Bonus (optional) sourmash lca gather Above, we ran sourmash lca gather on our untrimmed data. 94% of the sample did not contain sequence in any GenBank assembly. A substantial proportion of this sequence could be due to errors. Run sourmash lca gather again on your adapter and k-mer trimmed data. How much less of the sequence is unclassifiable when the errors and adapters are removed? How many species are no longer detected after k-mer and error trimming?","title":"Taxonomic Discovery with Sourmash"},{"location":"07_taxonomic_discovery_with_sourmash/#introduction-to-sourmash","text":"Please read this tutorial for an introduction to how sourmash works. tl;dr (but actually please read it): sourmash breaks nucleotide sequences down into small pieces, and then searches for those small pieces in databases. This makes it really fast to make comparisons. Here, we will compare our metagenome sample against a pre-prepared database that contains all microbial sequences in GenBank","title":"Introduction to sourmash"},{"location":"07_taxonomic_discovery_with_sourmash/#starting-with-sourmash","text":"Sourmash doesn't have a big memory or CPU footprint, and can be run on most laptops. Below is a recommended srun command to start an interactive session in which to run the srun commands. srun -p bmh -J sourmash24 -t 24:00:00 --mem=16gb -c 1 --pty bash Let's install sourmash conda activate dib_rotation conda install -y -c conda-forge -c bioconda sourmash Next, let's create a directory in which to store our sourmash results cd ~/2020_rotation_project mkdir -p sourmash cd sourmash Then we can link in our raw data. We could run sourmash with our adapter trimmed or k-mer trimmed data. In fact, doing so would make sourmash faster because there would be fewer k-mers in the sample. We are comparing our sample against a database of trusted DNA sequences. If we have adapters or errors in our reads, these won't match to the assemblies in the database. However, even though we very lightly trimmed our reads, there is a chance that we removed a very low abundance organism that was truly present in the sample. ln -s ~/2020_rotation_project/raw_data/*fastq.gz . We need to generate a signature using sourmash. A signature is a compressed representation of the k-mers in the sequence. Using this data structure instead of the reads makes sourmash much faster. sourmash compute -o SRR1976948.sig --merge SRR1976948 --scaled 2000 -k 21,31,51 --track-abundance SRR1976948_*fastq.gz The outputs file, SRR1976948.sig holds a representative subset of k-mers from our original sample, as well as their abundance information. The k-mers are \"hashed\", or transformed, into numbers to make selecting, storing, and looking up the k-mers more efficient.","title":"Starting with sourmash"},{"location":"07_taxonomic_discovery_with_sourmash/#sourmash-lca-gather","text":"Sourmash provides two methods for estimating the taxonomic composition of known sequences in a metagenome, sourmash gather and sourmash lca gather . sourmash gather gives strain-level specificity to matches in its output -- e.g. all strains that match any sequences (above a threshold) in your metagenome will be reported, along with the percent of each strain that matches. This is useful both to estimate the amount of your metagenome that is known, and to estimate the closest strain relative to the thing that is in your metagenome. sourmash lca gather provides a similar output, but also provides taxonomic information for the match. This means that both an assembly and a taxonomy are required for inclusion in the database. Because of the underlying data structures, sourmash lca gather is faster, but also requires more memory. Farm has plenty of resources, so we will run the faster command. Download and unzip the database: curl -L https://osf.io/4f8n3/download -o genbank-k31.lca.json.gz gunzip genbank-k31.lca.json.gz And then run lca gather sourmash lca gather -o SRR1976948_lca_gather.csv SRR1976948.sig genbank-k31.lca.json We see an output that looks like this: == This is sourmash version 3.0.1. == == Please cite Brown and Irber (2016), doi:10.21105/joss.00027. == loaded 1 LCA databases. ksize=31, scaled=10000 selecting specified query k=31 loaded query: SRR1976948... (k=31) overlap p_query p_match --------- ------- -------- 2.5 Mbp 0.2% 96.9% unassigned Methanomicrobiales archaeon 53_19 2.4 Mbp 0.2% 99.2% Methanobacterium sp. 42_16 2.3 Mbp 0.2% 100.0% Desulfotomaculum sp. 46_80 2.3 Mbp 0.2% 100.0% unassigned Actinobacteria bacterium 66_15 2.1 Mbp 0.2% 97.7% Desulfotomaculum sp. 46_296 2.1 Mbp 0.2% 99.0% Methanosaeta harundinacea 2.0 Mbp 0.2% 99.0% unassigned Marinimicrobia bacterium 46_43 1.9 Mbp 0.2% 100.0% unassigned Bacteroidetes bacterium 38_7 1.9 Mbp 0.2% 55.1% unassigned Thermotogales bacterium EBM-48 The first column estimates the amount of sequences in our metagenome that are contained in the match, while the second column estimates the amount of the match that is contained within our metagenome. These percentages are quite high in the p_match column...that's because the authors who originally analyzed this sample deposited metagenome-assembled genomes from this sample into GenBank. When sourmash is finished running, it tells us that 94% of our sequence was unclassified; i.e. it doesn't match any sequence in the database. This is common for metagenomics, particularly for samples that are sequenced from rare environments (like Alaskan oil reservoirs). In the next lesson, we will work to improve the percent of sequence in the metagenome that is classifiable.","title":"Sourmash lca gather"},{"location":"07_taxonomic_discovery_with_sourmash/#final-thoughts","text":"There are many tools like Kraken and Kaiju that can do taxonomic classification of individual reads from metagenomes; these seem to perform well (albeit with high false positive rates) in situations where you don\u2019t necessarily have the genome sequences that are in the metagenome. Sourmash, by contrast, can estimate which known genomes are actually present, so that you can extract them and map/align to them. It seems to have a very low false positive rate and is quite sensitive to strains.","title":"Final Thoughts"},{"location":"07_taxonomic_discovery_with_sourmash/#bonus-optional-sourmash-lca-gather","text":"Above, we ran sourmash lca gather on our untrimmed data. 94% of the sample did not contain sequence in any GenBank assembly. A substantial proportion of this sequence could be due to errors. Run sourmash lca gather again on your adapter and k-mer trimmed data. How much less of the sequence is unclassifiable when the errors and adapters are removed? How many species are no longer detected after k-mer and error trimming?","title":"Bonus (optional) sourmash lca gather"},{"location":"08_bin_completion_with_spacegraphcats/","text":"In the previous lesson , we used sourmash to determine the approximate taxonomic composition of our metagenome sample. Sourmash performs quick exact matching between the k-mers in your sample and k-mers in databases -- this means that a sequence must have been previously sequenced and be in a database in order for us to be able to label it in our sample. We often cannot label a lot of the sequences in our sample, especially if that sample comes from a novel environment that has not be sequenced in the past. We saw this in the previous lesson, where we were only able to classify ~10% of the reads in our sample. == This is sourmash version 3.0.1. == == Please cite Brown and Irber (2016), doi:10.21105/joss.00027. == loaded 1 LCA databases. ksize=31, scaled=10000 selecting specified query k=31 loaded query: SRR1976948... (k=31) overlap p_query p_match --------- ------- -------- 2.5 Mbp 0.2% 96.9% unassigned Methanomicrobiales archaeon 53_19 2.4 Mbp 0.2% 99.2% Methanobacterium sp. 42_16 2.3 Mbp 0.2% 100.0% Desulfotomaculum sp. 46_80 2.3 Mbp 0.2% 100.0% unassigned Actinobacteria bacterium 66_15 2.1 Mbp 0.2% 97.7% Desulfotomaculum sp. 46_296 2.1 Mbp 0.2% 99.0% Methanosaeta harundinacea 2.0 Mbp 0.2% 99.0% unassigned Marinimicrobia bacterium 46_43 1.9 Mbp 0.2% 100.0% unassigned Bacteroidetes bacterium 38_7 1.9 Mbp 0.2% 55.1% unassigned Thermotogales bacterium EBM-48 Analysis options for metagenome samples To get more information out of a metagenomics sample, we have five options. 1) Align reads to the genomes that had sourmash matches. sourmash performs exact matching of k-mers. K-mers of size 31 are fairly specific. Even if a sequence had 30 basepairs exactly in common with another sequence, if the 31st is different, it would not count as a match. Aligning reads to close relatives is a more lenient approach, and could lead to 5-10% more reads being classified. This is pretty good, but there are ways to do better. 2) de novo assemble and bin the reads into metagenome assembled genomes. de novo assembly and binning are reference-free approaches to produce metagenome-assembled genomes (bins) from metagenome reads. de novo assembly works by finding overlaps between reads and assembling them into larger \"contiguous sequences\" (usually shortened to contigs). Depending on the depth, coverage, and biological properties of a sample, these contigs range in size from 500 base pairs to hundreds of thousands of base pairs. These assemblies can then be binned into metagenome-assembled genomes. Most binners use tetranucleotide frequency and abundance information to bin contigs. A tetranucleotide is a 4 base pair sequence within a genome. Almost all tetranucleotides occur in almost all genomes, however the frequency that they occur in a given genome is usually conserved (see here ). So, binners exploit this information and calculate tetranucleotide frequency for all contigs in an assembly, and group the contigs together that have similar frequencies. This is also coupled with abundance information; if two contigs belong together, they probably have the same abundance because they came from the same organism in a sample. These approaches allow researchers to ask questions about the genomes of organisms in metagenomes even if there is no reference that has been sequenced before. For this reason, they are both very popular and very powerful. However, both assembly and binning suffer from biases that lead to incomplete results. Assembly fails when either 1) an area does not have enough reads to cover the region (low coverage) and 2) when the region is too complicated and there are too many viable combinations of sequences so the assembler doesn't know how to make a decision. This second scenario can occur when there are a lot of errors in the reads, or when there is a lot of strain variation in a genome. In either case, the assembly breaks and outputs fragmented contigs, or no contigs at all. Although tetranucleotide frequency and abundance information are strong signals, tetranucleotide frequency can only be reliably estimated on contigs that are >2000 base pairs. Because many things fail to assemble to that length, they are not binned. To give an idea of how much is missed by de novo assembly and binning, consider our sourmash results. The sample that we are analyzing was originally analyzed with a de novo assembly and binning pipeline. The high-quality bins were then uploaded to GenBank and are now part of the database. Look at the sourmash output (above). Any genome match that ends in two numbers separated by an underscore (e.g. 46_43) is a de novo metagenome-assembled genome produced by the original analysis. Even with the exact genomes in our sample in the database, we were only able to classify 10% of the k-mers in our sample. This leaves a lot of data on the table. 3) Continue the analysis with gene-level techniques. Often times, many more contigs will assemble than will bin. In cases like this, it's possible to do a gene-level analysis of a metagenome where you annotate open reading frames (ORFs) on the assembled contigs. This type of analysis can give you an idea of the functional potential in our metagenome. 4) Continue the analysis with read-based techniques. If something has no known reference and doesn't assemble or bin, what can you do? There are many tools that work directly on metagenome reads to estimate taxonomy or function (e.g. gene identity). These tools include Kraken and mifaser. We've had varying degrees of success with this type of approach, depending on the sample being analyzed. 5) Exploit connectivity of DNA sequences to assign more reads to the pangenome of sourmash matches. We can do this with a tool called spacegraphcats. The rest of this lesson covers background knowledge for this approach and why it works, and gives a step-by-step guide of how to do it. Representing all the k-mers: de Bruijn graphs and compact de Bruijn graphs In real life, DNA sequences are fully contiguous within a genome (or chromosome). When we chop a genome up into little pieces to sequence it, we lose this connectivity information. In order to get this connectivity information back to create an assembly, we must find every possible set of overlaps between all reads. This information is often represented in a de Bruijn graph. A de Bruijn graph is built by chopping sequencing data into k-mers and finding all overlaps of size k-1 between all k-mers. The graph below demonstrates this process. Each node in the graph represents a k-mer (here of size 4), and each arrows represents that two k-mers overlap by k-1 nucleotides (here, by 3 nucleotides). Each k-mer only occurs in the graph once. A compact de Bruijn graph (cDBG) is built from the de Bruijn graph by collapsing all linear paths of k-mers into a single node. Every k-mer still only occurs once in the graph, but now nodes grow in size to be greater than k. Both DBGs and cDBGs contain every k-mer from a sample. What's more, with a large enough k-mer size (e.g. 31), k-mers from the same genome tend to be connected within the graph because they are connected within the real genome. spacegraphcats takes advantage of this information to reassociate k-mers that belong together but were not recovered by assembly. Sequence context in a cDBG As outlined above, many reads do not match known references, don't assemble, and/or don't bin. What if this occurs in a region that you particularly care about? How can we use the fact that all k-mers in a metagenome are contained within a cDBG to get more information about our region of interest? Look at the figure below. The first part of the abstract of this study states: Dissimilatory perchlorate reduction is an anaerobic respiratory pathway that in communities might be influenced by metabolic interactions. Because the genes for perchlorate reduction are horizontally transferred, previous studies have been unable to identify uncultivated perchlorate-reducing populations. Here we recovered metagenome-assembled genomes from perchlorate-reducing sediment enrichments and employed a manual scaffolding approach to reconstruct gene clusters for perchlorate reduction found within mobile genetic elements. Essentially, the authors of this study knew that dissimilatory perchlorate reduction was important in their microbial communities of interest. But because perchlorate reduction genes are horizontally transferred, they're hard to identify in metagenomic sequencing data. This is likely because strain variation (e.g. small differences in the same genes in different organisms in a community) around these genes broke assembly. However, given their importance, the authors of this study knew that they should be present in the metagenome. Therefore, they dug into the cDBG that contained all metagenome reads, and were able to see the sequence context of their genes of interest. We see that because these genes are horizontally transferred, sometimes the sequence flanking their genes of interest comes from multiple species. However, even in this case, regions of the sub-cDBG segregate to each species that contains the gene of interest. In other cases (e.g. at 7% salinity), only one organism flanks pcrA . Yet we see from bubbles and offshoots in the graph that even when pcrA is contained only within one species, there is still variation that likely confounded assembly and binning. Scaling it up The study we were looking at above looked for genes involved in dissimilatory perchlorate reduction using manual curation -- e.g. they built a cDBG for each sample, visualized it with a tool called Bandage, then used BLAST to find their genes of interest. This approach is great, but incredibly time consuming and difficult to automate. What's more, this approach does not scale. Given the complexity of cDBGs, each \"query\" (e.g. look up within the graph) takes a lot of computational resources. As such, although this approach is tractable for a few genes (and shows really cool results at that scale!), it wouldn't work for every gene that is assembled in a metagenome. But imagine if it could! Using de novo approaches, most bins are not 100% complete. What if we could use a whole bin as a query, and pull out its context within a cDBG? We could \"complete\" a bin by pulling all the things that didn't assemble or bin, but that through close location in the cDBG, we know belongs with the bin. Enter spacegraphcats. Spacegraphcats uses a novel data structure to represent the cDBG with less complexity while maintaining biological relationships between the sequences. It then uses novel algorithms that exploit properties of the cDBG to quickly query into the data structure. To visualize this, let's look at a cDBG of an Escherichia coli genome + errors (this is an isolate, so we're using the errors to simulate strain variation in a real metagenome community. It's a rough approximation that works well for visualizing what spacegraphcats does under the hood). On the left is the cDBG, and on the right is the simplified structure produced by spacegraphcats. The structure on the right is much easier to query into. Spacegraphcats queries work by decomposing the query into k-mers, finding the node in which a query k-mer is contained within the spacegraphcats graph, and returning all of the k-mers in that node. This process is efficient enough to work on the whole metagenome for every k-mer in the query. Running spacegraphcats Let's try this out on our metagenome! First, let's install spacegraphcats. We're going to create a new environment for spacegraphcats, and follow the installation instructions from the spacegraphcats github repository. First, start an srun session srun -p bmh -J sgc -t 48:00:00 --mem=70gb -c 2 --pty bash Make sure you start from the base environment. If you're in another environment (e.g. dib_rotation ), run conda deactivate . cd ~ git clone https://github.com/spacegraphcats/spacegraphcats/ conda env create -f spacegraphcats/environment.yml -n sgc conda activate sgc pip install --pre spacegraphcats Next, we need to decide what query we'd like to use. Let's try Desulfotomaculum sp. 46_80 , which we saw from sourmash is 100% present in the metagenome 2.3 Mbp 0.2% 100.0% Desulfotomaculum sp. 46_80 We first have to find this genome sequence. To do this, we can go to NCBI taxonomy browser and search for Desulfotomaculum sp. 46_80 . We see that this takes us to a new page. On the right hand side of the screen, there should be a box. Click the 1 that's in the Assembly row, and this will take us to the genome bin assembly record. Then, on the right hand side, click FTP directory for GenBank assembly . This takes us to the FTP site for this assembly. We can download the assembly to FARM using wget and the link address for the file we want. Here, we want the *.fna.gz file, as this contains the nucleotides from the metagenome-assembled genome. cd ~/2020_rotation_project mkdir -p sgc cd sgc wget ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/001/508/995/GCA_001508995.1_ASM150899v1/GCA_001508995.1_ASM150899v1_genomic.fna.gz Now that we have our query, we need to construct a configuration file that spacegraphcats will use to run. This file specifies the name of the spacegraphcats data structure ( catlas_base ), the file paths for the metagenome reads and the query sequence, the k-mer size at which to build the cDBG, and the \"radius\" (or size) at which to perform the queries. catlas_base: 'SRR1976948' input_sequences: - ../abundtrim/SRR1976948.abundtrim.fq.gz ksize: 31 radius: 1 search: - GCA_001508995.1_ASM150899v1_genomic.fna.gz searchquick: GCA_001508995.1_ASM150899v1_genomic.fna.gz Use a text editor such as nano or vim to generate this file, and call it conf1.yml . (Here's a tutorial on using nano .) Now we're ready to run spacegraphcats! python -m spacegraphcats run conf1.yml extract_contigs extract_reads --nolock This will take a while to run. When it is finished, you will have three folders in your directory: + SRR1976948 : contains the cDBG + SRR1976948_k31_r1 : contains the spacegraphcats data structures + SRR1976948_k31_r1_search_oh0 : contains the output of the query, including the contigs (single paths) from the cDBG, the reads that contain k-mers in those contigs, and a sourmash signature from the contigs. Comparing the query to the neighborhood Let's explore how similar the query is to the neighborhood we extracted with it. We will use sourmash to do this. We already have a sourmash signature our neighborhood that was output by spacegraphcats. Let's make a signature for our query as well, and then use sourmash compare to compare them. sourmash compute -k 21,31,51 --scaled 2000 -o GCA_001508995.1_ASM150899v1_genomic.sig GCA_001508995.1_ASM150899v1_genomic.fna.gz sourmash compare -k 31 --csv comp1.csv *sig SRR1976948_k31_r1_search_oh0/*sig How similar are these two signatures? Did spacegraphcats add any additional sequence to this query?","title":"Bin Completion with Spacegraphcats"},{"location":"08_bin_completion_with_spacegraphcats/#analysis-options-for-metagenome-samples","text":"To get more information out of a metagenomics sample, we have five options. 1) Align reads to the genomes that had sourmash matches. sourmash performs exact matching of k-mers. K-mers of size 31 are fairly specific. Even if a sequence had 30 basepairs exactly in common with another sequence, if the 31st is different, it would not count as a match. Aligning reads to close relatives is a more lenient approach, and could lead to 5-10% more reads being classified. This is pretty good, but there are ways to do better. 2) de novo assemble and bin the reads into metagenome assembled genomes. de novo assembly and binning are reference-free approaches to produce metagenome-assembled genomes (bins) from metagenome reads. de novo assembly works by finding overlaps between reads and assembling them into larger \"contiguous sequences\" (usually shortened to contigs). Depending on the depth, coverage, and biological properties of a sample, these contigs range in size from 500 base pairs to hundreds of thousands of base pairs. These assemblies can then be binned into metagenome-assembled genomes. Most binners use tetranucleotide frequency and abundance information to bin contigs. A tetranucleotide is a 4 base pair sequence within a genome. Almost all tetranucleotides occur in almost all genomes, however the frequency that they occur in a given genome is usually conserved (see here ). So, binners exploit this information and calculate tetranucleotide frequency for all contigs in an assembly, and group the contigs together that have similar frequencies. This is also coupled with abundance information; if two contigs belong together, they probably have the same abundance because they came from the same organism in a sample. These approaches allow researchers to ask questions about the genomes of organisms in metagenomes even if there is no reference that has been sequenced before. For this reason, they are both very popular and very powerful. However, both assembly and binning suffer from biases that lead to incomplete results. Assembly fails when either 1) an area does not have enough reads to cover the region (low coverage) and 2) when the region is too complicated and there are too many viable combinations of sequences so the assembler doesn't know how to make a decision. This second scenario can occur when there are a lot of errors in the reads, or when there is a lot of strain variation in a genome. In either case, the assembly breaks and outputs fragmented contigs, or no contigs at all. Although tetranucleotide frequency and abundance information are strong signals, tetranucleotide frequency can only be reliably estimated on contigs that are >2000 base pairs. Because many things fail to assemble to that length, they are not binned. To give an idea of how much is missed by de novo assembly and binning, consider our sourmash results. The sample that we are analyzing was originally analyzed with a de novo assembly and binning pipeline. The high-quality bins were then uploaded to GenBank and are now part of the database. Look at the sourmash output (above). Any genome match that ends in two numbers separated by an underscore (e.g. 46_43) is a de novo metagenome-assembled genome produced by the original analysis. Even with the exact genomes in our sample in the database, we were only able to classify 10% of the k-mers in our sample. This leaves a lot of data on the table. 3) Continue the analysis with gene-level techniques. Often times, many more contigs will assemble than will bin. In cases like this, it's possible to do a gene-level analysis of a metagenome where you annotate open reading frames (ORFs) on the assembled contigs. This type of analysis can give you an idea of the functional potential in our metagenome. 4) Continue the analysis with read-based techniques. If something has no known reference and doesn't assemble or bin, what can you do? There are many tools that work directly on metagenome reads to estimate taxonomy or function (e.g. gene identity). These tools include Kraken and mifaser. We've had varying degrees of success with this type of approach, depending on the sample being analyzed. 5) Exploit connectivity of DNA sequences to assign more reads to the pangenome of sourmash matches. We can do this with a tool called spacegraphcats. The rest of this lesson covers background knowledge for this approach and why it works, and gives a step-by-step guide of how to do it.","title":"Analysis options for metagenome samples"},{"location":"08_bin_completion_with_spacegraphcats/#representing-all-the-k-mers-de-bruijn-graphs-and-compact-de-bruijn-graphs","text":"In real life, DNA sequences are fully contiguous within a genome (or chromosome). When we chop a genome up into little pieces to sequence it, we lose this connectivity information. In order to get this connectivity information back to create an assembly, we must find every possible set of overlaps between all reads. This information is often represented in a de Bruijn graph. A de Bruijn graph is built by chopping sequencing data into k-mers and finding all overlaps of size k-1 between all k-mers. The graph below demonstrates this process. Each node in the graph represents a k-mer (here of size 4), and each arrows represents that two k-mers overlap by k-1 nucleotides (here, by 3 nucleotides). Each k-mer only occurs in the graph once. A compact de Bruijn graph (cDBG) is built from the de Bruijn graph by collapsing all linear paths of k-mers into a single node. Every k-mer still only occurs once in the graph, but now nodes grow in size to be greater than k. Both DBGs and cDBGs contain every k-mer from a sample. What's more, with a large enough k-mer size (e.g. 31), k-mers from the same genome tend to be connected within the graph because they are connected within the real genome. spacegraphcats takes advantage of this information to reassociate k-mers that belong together but were not recovered by assembly.","title":"Representing all the k-mers: de Bruijn graphs and compact de Bruijn graphs"},{"location":"08_bin_completion_with_spacegraphcats/#sequence-context-in-a-cdbg","text":"As outlined above, many reads do not match known references, don't assemble, and/or don't bin. What if this occurs in a region that you particularly care about? How can we use the fact that all k-mers in a metagenome are contained within a cDBG to get more information about our region of interest? Look at the figure below. The first part of the abstract of this study states: Dissimilatory perchlorate reduction is an anaerobic respiratory pathway that in communities might be influenced by metabolic interactions. Because the genes for perchlorate reduction are horizontally transferred, previous studies have been unable to identify uncultivated perchlorate-reducing populations. Here we recovered metagenome-assembled genomes from perchlorate-reducing sediment enrichments and employed a manual scaffolding approach to reconstruct gene clusters for perchlorate reduction found within mobile genetic elements. Essentially, the authors of this study knew that dissimilatory perchlorate reduction was important in their microbial communities of interest. But because perchlorate reduction genes are horizontally transferred, they're hard to identify in metagenomic sequencing data. This is likely because strain variation (e.g. small differences in the same genes in different organisms in a community) around these genes broke assembly. However, given their importance, the authors of this study knew that they should be present in the metagenome. Therefore, they dug into the cDBG that contained all metagenome reads, and were able to see the sequence context of their genes of interest. We see that because these genes are horizontally transferred, sometimes the sequence flanking their genes of interest comes from multiple species. However, even in this case, regions of the sub-cDBG segregate to each species that contains the gene of interest. In other cases (e.g. at 7% salinity), only one organism flanks pcrA . Yet we see from bubbles and offshoots in the graph that even when pcrA is contained only within one species, there is still variation that likely confounded assembly and binning.","title":"Sequence context in a cDBG"},{"location":"08_bin_completion_with_spacegraphcats/#scaling-it-up","text":"The study we were looking at above looked for genes involved in dissimilatory perchlorate reduction using manual curation -- e.g. they built a cDBG for each sample, visualized it with a tool called Bandage, then used BLAST to find their genes of interest. This approach is great, but incredibly time consuming and difficult to automate. What's more, this approach does not scale. Given the complexity of cDBGs, each \"query\" (e.g. look up within the graph) takes a lot of computational resources. As such, although this approach is tractable for a few genes (and shows really cool results at that scale!), it wouldn't work for every gene that is assembled in a metagenome. But imagine if it could! Using de novo approaches, most bins are not 100% complete. What if we could use a whole bin as a query, and pull out its context within a cDBG? We could \"complete\" a bin by pulling all the things that didn't assemble or bin, but that through close location in the cDBG, we know belongs with the bin. Enter spacegraphcats. Spacegraphcats uses a novel data structure to represent the cDBG with less complexity while maintaining biological relationships between the sequences. It then uses novel algorithms that exploit properties of the cDBG to quickly query into the data structure. To visualize this, let's look at a cDBG of an Escherichia coli genome + errors (this is an isolate, so we're using the errors to simulate strain variation in a real metagenome community. It's a rough approximation that works well for visualizing what spacegraphcats does under the hood). On the left is the cDBG, and on the right is the simplified structure produced by spacegraphcats. The structure on the right is much easier to query into. Spacegraphcats queries work by decomposing the query into k-mers, finding the node in which a query k-mer is contained within the spacegraphcats graph, and returning all of the k-mers in that node. This process is efficient enough to work on the whole metagenome for every k-mer in the query.","title":"Scaling it up"},{"location":"08_bin_completion_with_spacegraphcats/#running-spacegraphcats","text":"Let's try this out on our metagenome! First, let's install spacegraphcats. We're going to create a new environment for spacegraphcats, and follow the installation instructions from the spacegraphcats github repository. First, start an srun session srun -p bmh -J sgc -t 48:00:00 --mem=70gb -c 2 --pty bash Make sure you start from the base environment. If you're in another environment (e.g. dib_rotation ), run conda deactivate . cd ~ git clone https://github.com/spacegraphcats/spacegraphcats/ conda env create -f spacegraphcats/environment.yml -n sgc conda activate sgc pip install --pre spacegraphcats Next, we need to decide what query we'd like to use. Let's try Desulfotomaculum sp. 46_80 , which we saw from sourmash is 100% present in the metagenome 2.3 Mbp 0.2% 100.0% Desulfotomaculum sp. 46_80 We first have to find this genome sequence. To do this, we can go to NCBI taxonomy browser and search for Desulfotomaculum sp. 46_80 . We see that this takes us to a new page. On the right hand side of the screen, there should be a box. Click the 1 that's in the Assembly row, and this will take us to the genome bin assembly record. Then, on the right hand side, click FTP directory for GenBank assembly . This takes us to the FTP site for this assembly. We can download the assembly to FARM using wget and the link address for the file we want. Here, we want the *.fna.gz file, as this contains the nucleotides from the metagenome-assembled genome. cd ~/2020_rotation_project mkdir -p sgc cd sgc wget ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/001/508/995/GCA_001508995.1_ASM150899v1/GCA_001508995.1_ASM150899v1_genomic.fna.gz Now that we have our query, we need to construct a configuration file that spacegraphcats will use to run. This file specifies the name of the spacegraphcats data structure ( catlas_base ), the file paths for the metagenome reads and the query sequence, the k-mer size at which to build the cDBG, and the \"radius\" (or size) at which to perform the queries. catlas_base: 'SRR1976948' input_sequences: - ../abundtrim/SRR1976948.abundtrim.fq.gz ksize: 31 radius: 1 search: - GCA_001508995.1_ASM150899v1_genomic.fna.gz searchquick: GCA_001508995.1_ASM150899v1_genomic.fna.gz Use a text editor such as nano or vim to generate this file, and call it conf1.yml . (Here's a tutorial on using nano .) Now we're ready to run spacegraphcats! python -m spacegraphcats run conf1.yml extract_contigs extract_reads --nolock This will take a while to run. When it is finished, you will have three folders in your directory: + SRR1976948 : contains the cDBG + SRR1976948_k31_r1 : contains the spacegraphcats data structures + SRR1976948_k31_r1_search_oh0 : contains the output of the query, including the contigs (single paths) from the cDBG, the reads that contain k-mers in those contigs, and a sourmash signature from the contigs.","title":"Running spacegraphcats"},{"location":"08_bin_completion_with_spacegraphcats/#comparing-the-query-to-the-neighborhood","text":"Let's explore how similar the query is to the neighborhood we extracted with it. We will use sourmash to do this. We already have a sourmash signature our neighborhood that was output by spacegraphcats. Let's make a signature for our query as well, and then use sourmash compare to compare them. sourmash compute -k 21,31,51 --scaled 2000 -o GCA_001508995.1_ASM150899v1_genomic.sig GCA_001508995.1_ASM150899v1_genomic.fna.gz sourmash compare -k 31 --csv comp1.csv *sig SRR1976948_k31_r1_search_oh0/*sig How similar are these two signatures? Did spacegraphcats add any additional sequence to this query?","title":"Comparing the query to the neighborhood"},{"location":"09_assembling_a_nbhd/","text":"In the previous lesson , we observed that querying our metagenome with a de novo assembled genome returned a neighborhood that was 55% similar to the query. Now we will assemble our query neighborhood and determine whether it contains additional amino acid sequences that were not in our query. But wasn't assembly a problem? Yes! As we discussed last lesson, de novo assembly and binning are often part of the problem -- assemblers don't pick a path when they're faced with too many options, and output small, fragmented contigs instead. Look at the graphic below. It depicts some common structures in cDBGs that can lead to breaks in contigs. A lot of variation that occurs in sequences occurs at the nucleotide level. This is in part attributable to third base pair wobble , which leads to silent variation in nucleotide sequences that confounds nucleotide assembly. One way to circumvent assembly problems that arise from sequence variation is to assemble in amino acid space. Amino acid assemblers translate nucleotide reads into amino acid space, and then find overlaps between the translated amino acid sequences to assemble open reading frames. In the context of metagenomes, this results in many more assembled open reading frames that can then be analyzed for their functional potential. We will use the PLASS amino acid assembler to assemble our query neighborhood. Then, we will compare the proteins we assembled with PLASS to those in the original query. Running PLASS and formatting the output First, start an srun session srun -p bmh -J plass -t 24:00:00 --mem=8gb -c 1 --pty bash Then, we can install PLASS into our dib_rotation environment. conda activate dib_rotation conda install plass Now we can run PLASS! cd ~/2020_rotation_project mkdir -p plass cd plass plass assemble ../sgc/SRR1976948_k31_r1_search_oh0/GCA_001508995.1_ASM150899v1_genomic.fna.gz.cdbg_ids.reads.gz query_nbhd_plass.fa tmp When PLASS finishes, we have to do quite a bit of formatting. First, PLASS adds * to the end of each amino acid sequence to indicate stop codons. Most tools don't recognize this as a valid amino acid encoding, so we have to remove this character. We'll download a script and then run it to remove this stop codon. wget https://raw.githubusercontent.com/spacegraphcats/2018-paper-spacegraphcats/master/pipeline-base/scripts/remove-stop-plass.py python remove-stop-plass.py query_nbhd_plass.fa Next, PLASS also outputs identical amino acid sequences when the underlying nucleotide sequences that led to the amino acid sequences are different. These are redundant and we don't need them, so we can remove them using a tool called CD-HIT. CD-HIT clusters sequences at a user-specified identity, and selects a representative sequence for each cluster. In our case, we can cluster at 100% identity and that will reduce the number of amino acid sequences in our final output file. First, install CD-HIT. Make sure you're in your dib_rotation environment. If you're not, run conda activate dib_rotation . conda install cd-hit Then run CD-HIT cd-hit -c 1 -i query_nbhd_plass.fa.nostop.fa -o query_nbhd_plass.cdhit.fa Comparing amino acid sequences in our neighborhood to those in the query Let's compare the amino acid sequences in our neighborhood to those in the query. First, download amino acid sequences in the query. Not every GenBank record has translated amino acid sequences, but because ours does, we can use them. cd ~/2020_rotation_project mkdir -p blast cd blast wget ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/001/508/995/GCA_001508995.1_ASM150899v1/GCA_001508995.1_ASM150899v1_protein.faa.gz gunzip GCA_001508995.1_ASM150899v1_protein.faa.gz There are many, many ways that we could compare the amino acid sequences between the two sequences. We could use sourmash compute with the --protein flag to generate signatures for both of our amino acid fasta files, and then use sourmash compare to compare them. Since we've used sourmash quite a bit, let's take a different approach that will give us a more detailed answer. We'll use BLAST to compare all of the protein sequences in our PLASS assembly to all of the protein sequences in the GenBank assembly. We just downloaded the GenBank assembly amino acid sequences to our blast folder, let's link in our PLASS amino acid assembly. ln -s ../plass/query_nbhd_plass.cdhit.fa . Next, we need to install BLAST into our environment. Make sure you're in your dib_rotation environment. If you're not, run conda activate dib_rotation . conda install blast Now we can use BLAST to compare our two sequences! BLAST has many options for comparing sequences. blastn compares nucleotide sequences against nucleotide squences. blastp compares protein sequences against protein sequences. blastx compares nucleotide sequences against protein sequences. tblastn compares protein sequences against nucleotide sequences. We'll use blastp since we are dealing with two amino acid fasta files. We'll also use the -query / -subject format for our BLAST command instead of using a BLAST database. This will allow us to BLAST two sequences together without needing to build our own database. (If you would like to learn how to build a BLAST database, see this tutorial ). We will also use the flag -outfmt 6 which will give us our results in tab-delineated format. blastp -query query_nbhd_plass.cdhit.fa -subject GCA_001508995.1_ASM150899v1_protein.faa -outfmt 6 -out query_nbhd_blast.tab Now that we have our BLAST results, we can analyze them in R. (Feel free to re-implement a similar approach in python if you don't like R!) We are looking for matches that are 100% identical between our query and our subject. R has a lot of dependencies. Let's make a new environment to keep those packages separate from our other tools. We'll install two R packages, which will bring along all of the R dependencies. dplyr has a set of functions for manipulating and formatting dataframes (like our BLAST table), and Biostrings makes reading amino acid sequences simple. conda deactivate conda create -n renv r-dplyr=0.8.3 bioconductor-biostrings=2.54.0 Once installed, activate the new environment conda activate renv and then start an R session by running: R Once inside of the R session, you should see a new prompt that looks like this: > Run the following code to analyze the BLAST results. Look at the comments to see what each line of code does. library(Biostrings) # import biostrings functions into current environment library(dplyr) # import dplyr functions into current environment nbhd <- readAAStringSet(\"query_nbhd_plass.cdhit.fa\") # import the plass nbhd nbhd_aas <- length(nbhd) # get number of AAs in nbhd blast <- read.table(\"query_nbhd_blast.tab\") # import blast results query_aas <- length(readAAStringSet(\"GCA_001508995.1_ASM150899v1_protein.faa\")) blast_100 <- filter(blast, V3 == 100) # retain only AAs that were 100% aas_100 <- length(unique(blast_100$V2)) # count num aas 100% contained aas_100/nbhd_aas # calculate the percent of AAs from the nbhd that were in the query aas_100/query_aas # calculate the percent of query that was in the nbhd How many amino acid sequences were added by the neighborhood query and the PLASS assembly? To quit R, type quit() Type n when asked Save workspace image?","title":"Assembling a Nbhd"},{"location":"09_assembling_a_nbhd/#but-wasnt-assembly-a-problem","text":"Yes! As we discussed last lesson, de novo assembly and binning are often part of the problem -- assemblers don't pick a path when they're faced with too many options, and output small, fragmented contigs instead. Look at the graphic below. It depicts some common structures in cDBGs that can lead to breaks in contigs. A lot of variation that occurs in sequences occurs at the nucleotide level. This is in part attributable to third base pair wobble , which leads to silent variation in nucleotide sequences that confounds nucleotide assembly. One way to circumvent assembly problems that arise from sequence variation is to assemble in amino acid space. Amino acid assemblers translate nucleotide reads into amino acid space, and then find overlaps between the translated amino acid sequences to assemble open reading frames. In the context of metagenomes, this results in many more assembled open reading frames that can then be analyzed for their functional potential. We will use the PLASS amino acid assembler to assemble our query neighborhood. Then, we will compare the proteins we assembled with PLASS to those in the original query.","title":"But wasn't assembly a problem?"},{"location":"09_assembling_a_nbhd/#running-plass-and-formatting-the-output","text":"First, start an srun session srun -p bmh -J plass -t 24:00:00 --mem=8gb -c 1 --pty bash Then, we can install PLASS into our dib_rotation environment. conda activate dib_rotation conda install plass Now we can run PLASS! cd ~/2020_rotation_project mkdir -p plass cd plass plass assemble ../sgc/SRR1976948_k31_r1_search_oh0/GCA_001508995.1_ASM150899v1_genomic.fna.gz.cdbg_ids.reads.gz query_nbhd_plass.fa tmp When PLASS finishes, we have to do quite a bit of formatting. First, PLASS adds * to the end of each amino acid sequence to indicate stop codons. Most tools don't recognize this as a valid amino acid encoding, so we have to remove this character. We'll download a script and then run it to remove this stop codon. wget https://raw.githubusercontent.com/spacegraphcats/2018-paper-spacegraphcats/master/pipeline-base/scripts/remove-stop-plass.py python remove-stop-plass.py query_nbhd_plass.fa Next, PLASS also outputs identical amino acid sequences when the underlying nucleotide sequences that led to the amino acid sequences are different. These are redundant and we don't need them, so we can remove them using a tool called CD-HIT. CD-HIT clusters sequences at a user-specified identity, and selects a representative sequence for each cluster. In our case, we can cluster at 100% identity and that will reduce the number of amino acid sequences in our final output file. First, install CD-HIT. Make sure you're in your dib_rotation environment. If you're not, run conda activate dib_rotation . conda install cd-hit Then run CD-HIT cd-hit -c 1 -i query_nbhd_plass.fa.nostop.fa -o query_nbhd_plass.cdhit.fa","title":"Running PLASS and formatting the output"},{"location":"09_assembling_a_nbhd/#comparing-amino-acid-sequences-in-our-neighborhood-to-those-in-the-query","text":"Let's compare the amino acid sequences in our neighborhood to those in the query. First, download amino acid sequences in the query. Not every GenBank record has translated amino acid sequences, but because ours does, we can use them. cd ~/2020_rotation_project mkdir -p blast cd blast wget ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/001/508/995/GCA_001508995.1_ASM150899v1/GCA_001508995.1_ASM150899v1_protein.faa.gz gunzip GCA_001508995.1_ASM150899v1_protein.faa.gz There are many, many ways that we could compare the amino acid sequences between the two sequences. We could use sourmash compute with the --protein flag to generate signatures for both of our amino acid fasta files, and then use sourmash compare to compare them. Since we've used sourmash quite a bit, let's take a different approach that will give us a more detailed answer. We'll use BLAST to compare all of the protein sequences in our PLASS assembly to all of the protein sequences in the GenBank assembly. We just downloaded the GenBank assembly amino acid sequences to our blast folder, let's link in our PLASS amino acid assembly. ln -s ../plass/query_nbhd_plass.cdhit.fa . Next, we need to install BLAST into our environment. Make sure you're in your dib_rotation environment. If you're not, run conda activate dib_rotation . conda install blast Now we can use BLAST to compare our two sequences! BLAST has many options for comparing sequences. blastn compares nucleotide sequences against nucleotide squences. blastp compares protein sequences against protein sequences. blastx compares nucleotide sequences against protein sequences. tblastn compares protein sequences against nucleotide sequences. We'll use blastp since we are dealing with two amino acid fasta files. We'll also use the -query / -subject format for our BLAST command instead of using a BLAST database. This will allow us to BLAST two sequences together without needing to build our own database. (If you would like to learn how to build a BLAST database, see this tutorial ). We will also use the flag -outfmt 6 which will give us our results in tab-delineated format. blastp -query query_nbhd_plass.cdhit.fa -subject GCA_001508995.1_ASM150899v1_protein.faa -outfmt 6 -out query_nbhd_blast.tab Now that we have our BLAST results, we can analyze them in R. (Feel free to re-implement a similar approach in python if you don't like R!) We are looking for matches that are 100% identical between our query and our subject. R has a lot of dependencies. Let's make a new environment to keep those packages separate from our other tools. We'll install two R packages, which will bring along all of the R dependencies. dplyr has a set of functions for manipulating and formatting dataframes (like our BLAST table), and Biostrings makes reading amino acid sequences simple. conda deactivate conda create -n renv r-dplyr=0.8.3 bioconductor-biostrings=2.54.0 Once installed, activate the new environment conda activate renv and then start an R session by running: R Once inside of the R session, you should see a new prompt that looks like this: > Run the following code to analyze the BLAST results. Look at the comments to see what each line of code does. library(Biostrings) # import biostrings functions into current environment library(dplyr) # import dplyr functions into current environment nbhd <- readAAStringSet(\"query_nbhd_plass.cdhit.fa\") # import the plass nbhd nbhd_aas <- length(nbhd) # get number of AAs in nbhd blast <- read.table(\"query_nbhd_blast.tab\") # import blast results query_aas <- length(readAAStringSet(\"GCA_001508995.1_ASM150899v1_protein.faa\")) blast_100 <- filter(blast, V3 == 100) # retain only AAs that were 100% aas_100 <- length(unique(blast_100$V2)) # count num aas 100% contained aas_100/nbhd_aas # calculate the percent of AAs from the nbhd that were in the query aas_100/query_aas # calculate the percent of query that was in the nbhd How many amino acid sequences were added by the neighborhood query and the PLASS assembly? To quit R, type quit() Type n when asked Save workspace image?","title":"Comparing amino acid sequences in our neighborhood to those in the query"},{"location":"10_annotating_amino_acid_sequences/","text":"In the previous lesson , we saw that our neighborhood query + PLASS assembly approach increased the number of amino acid sequences over those in our query by 26%. What do these amino acid sequences encode? Are they strain variants (e.g. a few amino acids different) from those that were already in the query, or do they add new functionality? In this lesson, we will annotate the amino acid sequences in our PLASS assembly and in the GenBank assembly, and then compare the encoded functions. As with most things in bioinformatics, there are many ways to annotate amino acid sequences. We will be using kofamscan , a tool that uses hidden markov models built from KEGG orthologs to assign KEGG ortholog numbers to new amino acid sequences. Hidden markov models work well for protein annotation because they weight the importance of each amino acid in determining the final assignment. Look at the figure below. This figure is a logo depicting the PFAM HMM for rpsG. rpsG encodes 30S ribosomal protein S7, and it is a highly conserved protein. The HMM was built from hundreds of rpsG protein sequences. At each position of the protein, the logo depicts the liklihood of seeing a specific amino acid. The larger the amino acid is in the logo, the more likely it is to be observed at that position. In positions where no amino acid is visible, it is less important which amino acid occurs there. This encoding is more flexible than something like Hamming distance or BLAST because it incorporates biological importance of amino acid positionality. This approach works well on novel amino acid sequences that are not closely related to anything currently housed in databases. kofamscan is a tool released by the KEGG that includes HMMs built from each KEGG ortholog. Using kofamscan, we can assign KEGG orthologs to our amino acid sequences. This allows us to take advantage of KEGG pathway information to determine if any pathways are more complete in our neighborhood than in our query. Running kofamscan First, start an srun session. srun -p bmh -J KO -t 48:00:00 --mem=8gb -c 6 --pty bash kofamscan is not available as a conda package, but being able to use the KEGG framework is important enough to deal with installing the kofamscan package manually. We'll create a new environment for kofamscan, and use conda to install all of its dependencies. conda create -n kofamscan hmmer parallel ruby kofamscan conda activate kofamscan Now, check that the installation worked properly by running kofamscan with the --help option: exec_annotation --help exec_annotation is the name of the script that runs kofamscan cd ~/2020_rotation_project mkdir -p kofamscan cd kofamscan ln -s ../plass/query_nbhd_plass.cdhit.fa . ln -s ../blast/GCA_001508995.1_ASM150899v1_protein.faa . Then, download the databases and executables for the kofamscan program. wget ftp://ftp.genome.jp/pub/db/kofam/ko_list.gz # download the ko list wget ftp://ftp.genome.jp/pub/db/kofam/profiles.tar.gz # download the hmm profiles Unzip and untar the relevant files: gunzip ko_list.gz tar xf profiles.tar.gz kofamscan runs using a config file. Using nano or vim (e.g. nano config.yml , build a config file that looks like this: # Path to your KO-HMM database # A database can be a .hmm file, a .hal file or a directory in which # .hmm files are. Omit the extension if it is .hal or .hmm file profile: ./profiles # Path to the KO list file ko_list: ko_list # Path to an executable file of hmmsearch # You do not have to set this if it is in your $PATH # hmmsearch: /usr/local/bin/hmmsearch # Path to an executable file of GNU parallel # You do not have to set this if it is in your $PATH # parallel: /usr/local/bin/parallel # Number of hmmsearch processes to be run parallelly cpu: 6 Now we can run kofamscan! We'll run it on our PLASS assembly, and then we will run it on the GenBank assembly. exec_annotation -f mapper --config config.yml -o query_nbhd_plass.clean_kofamscan.txt query_nbhd_plass.cdhit.fa exec_annotation -f mapper --config config.yml -o GCA_001508995.1_ASM150899v1_protein_kofamscan.txt GCA_001508995.1_ASM150899v1_protein.faa This will output two files that look something like this. The protein name occurs in the left column; this is derived from the amino acid header names. If a KEGG ortholog was able to be assigned to the protein, it occurs in the right column. NORP88_1 NORP88_2 NORP88_3 K01869 NORP88_4 NORP88_5 NORP88_6 K07082 NORP88_7 NORP88_8 K07448 NORP88_9 NORP88_10 K07507 Lastly, use KEGGDecoder to visualize the differences in KEGG pathways. conda install pip pip install KEGGDecoder Combine the kofamscan output files cat *_kofamscan.txt > kofamscan_results.txt And run KEGGDecoder KEGG-decoder -i kofamscan_results.txt -o kegg_decoder_out --vizoption static Transfer the output to your local computer using scp , and look at the results! Challenge: metabolishmm Above, we use kofamscan to assign KEGG orthologs to our amino acid sequences. There are other tools that would allow us look at the metabolic capabilities encoded in amino acid sequences. One such tool is metabolishm. Try using the metabolishmm tutorial and wiki documentation to install metabolishmm into its own environment, and to run the tool on our neighborhood and our query. Side note - using prokka to generate amino acid sequences from nucleotide sequences So far, we've been using the amino acid sequences from GenBank. What if we were working with a genome or bin that did not have that did not have amino acid sequences associated with it yet? We can generate amino acid sequences from nucleotide sequences, as well as first-pass annotations, using a tool called prokka . We can use conda to install prokka: conda install prokka To run prokka on a bin derived from a metagenome, use: prokka {input} --outdir {outdir} --prefix {bin} --metagenome --force --locustag {bin}","title":"Annotating amino acid sequences"},{"location":"10_annotating_amino_acid_sequences/#running-kofamscan","text":"First, start an srun session. srun -p bmh -J KO -t 48:00:00 --mem=8gb -c 6 --pty bash kofamscan is not available as a conda package, but being able to use the KEGG framework is important enough to deal with installing the kofamscan package manually. We'll create a new environment for kofamscan, and use conda to install all of its dependencies. conda create -n kofamscan hmmer parallel ruby kofamscan conda activate kofamscan Now, check that the installation worked properly by running kofamscan with the --help option: exec_annotation --help exec_annotation is the name of the script that runs kofamscan cd ~/2020_rotation_project mkdir -p kofamscan cd kofamscan ln -s ../plass/query_nbhd_plass.cdhit.fa . ln -s ../blast/GCA_001508995.1_ASM150899v1_protein.faa . Then, download the databases and executables for the kofamscan program. wget ftp://ftp.genome.jp/pub/db/kofam/ko_list.gz # download the ko list wget ftp://ftp.genome.jp/pub/db/kofam/profiles.tar.gz # download the hmm profiles Unzip and untar the relevant files: gunzip ko_list.gz tar xf profiles.tar.gz kofamscan runs using a config file. Using nano or vim (e.g. nano config.yml , build a config file that looks like this: # Path to your KO-HMM database # A database can be a .hmm file, a .hal file or a directory in which # .hmm files are. Omit the extension if it is .hal or .hmm file profile: ./profiles # Path to the KO list file ko_list: ko_list # Path to an executable file of hmmsearch # You do not have to set this if it is in your $PATH # hmmsearch: /usr/local/bin/hmmsearch # Path to an executable file of GNU parallel # You do not have to set this if it is in your $PATH # parallel: /usr/local/bin/parallel # Number of hmmsearch processes to be run parallelly cpu: 6 Now we can run kofamscan! We'll run it on our PLASS assembly, and then we will run it on the GenBank assembly. exec_annotation -f mapper --config config.yml -o query_nbhd_plass.clean_kofamscan.txt query_nbhd_plass.cdhit.fa exec_annotation -f mapper --config config.yml -o GCA_001508995.1_ASM150899v1_protein_kofamscan.txt GCA_001508995.1_ASM150899v1_protein.faa This will output two files that look something like this. The protein name occurs in the left column; this is derived from the amino acid header names. If a KEGG ortholog was able to be assigned to the protein, it occurs in the right column. NORP88_1 NORP88_2 NORP88_3 K01869 NORP88_4 NORP88_5 NORP88_6 K07082 NORP88_7 NORP88_8 K07448 NORP88_9 NORP88_10 K07507 Lastly, use KEGGDecoder to visualize the differences in KEGG pathways. conda install pip pip install KEGGDecoder Combine the kofamscan output files cat *_kofamscan.txt > kofamscan_results.txt And run KEGGDecoder KEGG-decoder -i kofamscan_results.txt -o kegg_decoder_out --vizoption static Transfer the output to your local computer using scp , and look at the results!","title":"Running kofamscan"},{"location":"10_annotating_amino_acid_sequences/#challenge-metabolishmm","text":"Above, we use kofamscan to assign KEGG orthologs to our amino acid sequences. There are other tools that would allow us look at the metabolic capabilities encoded in amino acid sequences. One such tool is metabolishm. Try using the metabolishmm tutorial and wiki documentation to install metabolishmm into its own environment, and to run the tool on our neighborhood and our query. Side note - using prokka to generate amino acid sequences from nucleotide sequences So far, we've been using the amino acid sequences from GenBank. What if we were working with a genome or bin that did not have that did not have amino acid sequences associated with it yet? We can generate amino acid sequences from nucleotide sequences, as well as first-pass annotations, using a tool called prokka . We can use conda to install prokka: conda install prokka To run prokka on a bin derived from a metagenome, use: prokka {input} --outdir {outdir} --prefix {bin} --metagenome --force --locustag {bin}","title":"Challenge: metabolishmm"},{"location":"11_workflows_and_repeatability/","text":"Workflows, Automation, and Repeatability For everything we have done so far, we have copied and pasted a lot of commands to accomplish what we want. This works! But can also be time consuming, and is more prone to error. We will show you next how to put all of these commands into a shell script. A shell script is a text file full of shell commands, that run just as if you're running them interactively at the command line. Writing a shell script Let's put some of our commands from the quality trimming module into one script. We'll call it run_qc.sh . The sh at the end of the tells you that this is a bash script. -First, cd into the 2020_rotation_project directory cd ~/2020_rotation_project Now, use nano to create and edit a file called run-qc.sh nano run-qc.sh will open the file. Now add the following text: cd ~/2020_rotation_project mkdir -p quality cd quality ln -s ~/2020_rotation_project/raw_data/*.fastq.gz ./ printf \"I see $(ls -1 *.fastq.gz | wc -l) files here.\\n\" for infile in *_1.fastq.gz do name=$(basename ${infile} _1.fastq.gz) fastp --in1 ${name}_1.fastq.gz --in2 ${name}_2.fastq.gz --out1 ${name}_1.trim.fastq.gz --out2 ${name}_2.trim.fastq.gz --detect_adapter_for_pe \\ --qualified_quality_phred 4 --length_required 31 --correction --json ${name}.trim.json --html ${name}.trim.html done This is now a shell script that you can use to execute all of those commands in one go, including running fastp on all six samples! Exit nano and try it out! Run: cd ~/2020_rotation_project bash run-qc.sh Re-running the shell script Suppose you wanted to re-run the script. How would you do that? Well, note that the quality directory is created at the top of the script, and everything is executed in that directory. So if you remove the quality directory like so, rm -rf quality The -rf here means that you'd like to remove the whole directory \"recursively\" ( r ) and that you'd like file deltion to happen without asking for permission for each file ( f ) You can then do: bash run-qc.sh Some tricks for writing shell scripts Make it executable You can get rid of the bash part of the command above with some magic: Put #! /bin/bash at the top of the file, and then run chmod +x ~/2020_rotation_project/run-qc.sh at the command line. You can now run ./run-qc.sh instead of bash run-qc.sh . You might be thinking, ok, why is this important? Well, you can do the same with R scripts and Python scripts (but put /usr/bin/env Rscript or /usr/bin/env python at the top, instead of /bin/bash ). This basically annotates the script with the language it's written in, so you don't have to know or remember yourself. So: it's not necessary but it's a nice trick. You can also always force a script to be run in a particular language by specifying bash <scriptname> or Rscript <Scriptname> , too. Automation with Workflow Systems! Automation via shell script is wonderful, but there are a few problems here. First, you have to run the entire workflow each time and it recomputes everything every time. If you're running a workflow that takes 4 days, and you change a command at the end, you'll have to manually go in and just run the stuff that depends on the changed command. Second, it's very explicit and not very generalizable . If you want to run it on a different dataset, you're going to have to change a lot of commands. You can read more about using workflow systems to streamline data-intensive biology in our preprint here . Snakemake Snakemake is one of several workflow systems that help solve these problems. If you want to learn snakemake, we recommend working through a tutorial, such as the one here . It's also worth checking out the snakemake documentation here . Here, we'll demo how to run the same steps above, but in Snakemake. First, let's install snakemake in our conda environment: conda install -y snakemake-minimal We're going to automate the same set of commands for trimming, but in snakemake. Open a file called Snakefile using nano : nano Snakefile Here is the command we would need for a single sample, SRR1976948 rule all: input: \"quality/SRR1976948_1.trim.fastq.gz\", \"quality/SRR1976948_2.trim.fastq.gz\" rule trim_reads: input: in1=\"raw_data/SRR1976948_1.fastq.gz\", in2=\"raw_data/SRR1976948_2.fastq.gz\", output: out1=\"quality/SRR1976948_1.trim.fastq.gz\", out2=\"quality/SRR1976948_2.trim.fastq.gz\", json=\"quality/SRR1976948.fastp.json\", html=\"quality/SRR1976948.fastp.html\" shell: \"\"\" fastp --in1 {input.in1} --in2 {input.in2} \\ --out1 {output.out1} --out2 {output.out2} \\ --detect_adapter_for_pe --qualified_quality_phred 4 \\ --length_required 31 --correction \\ --json {output.json} --html {output.html} \"\"\" We can run it like this: snakemake -n the -n tells snakemake to run a \"dry run\" - that is, just check that the input files exist and all files specified in rule all can be created from the rules provided within the Snakefile). you should see \"Nothing to be done.\" That's because the trimmed files already exist! Let's fix that: rm quality/SRR1976948*.trim.fastq.gz and now, when you run snakemake , you should see the fastp being run. Yay w00t! Then if you run snakemake again, you will see that it doesn't need to do anything - all the files are \"up to date\". Running all files at once Snakemake wouldn't be very useful if it could only trim one file at a time, so let's modify the Snakefile so it could run more files at once: SAMPLES = [\"SRR1976948\"] rule all: input: expand(\"quality/{sample}_1.trim.fastq.gz\", sample=SAMPLES) expand(\"quality/{sample}_2.trim.fastq.gz\", sample=SAMPLES) rule trim_reads: input: in1=\"raw_data/{sample}_R1.fastq.gz\", in2=\"raw_data/{sample}_R2.fastq.gz\", output: out1=\"quality/{sample}_1.trim.fastq.gz\", out2=\"quality/{sample}_2.trim.fastq.gz\", json=\"quality/{sample}.fastp.json\", html=\"quality/{sample}.fastp.html\" shell: \"\"\" fastp --in1 {input.in1} --in2 {input.in2} \\ --out1 {output.out1} --out2 {output.out2} \\ --detect_adapter_for_pe --qualified_quality_phred 4 \\ --length_required 31 --correction \\ --json {output.json} --html {output.html} \"\"\" Try another dryrun: snakemake -n Now actually run the workflow: snakemake -j 1 the -j 1 tells snakemake to run a single job at a time. You can increase this number if you have access to more cpu (e.g. you're in an srun session where you asked for more cpu with the -n parameter). Again, we see there's nothing to be done - the files exist! Try removing the quality trimmed files and running again. rm quality/*.trim.fastq.gz Adding an environment We've been using a conda environment throughout our modules. We can export the installed package names to a file that we can use to re-install all packages in a single step (like on a different computer). conda env export -n dib_rotation -f ~/2020_rotation_project/dib_rotation_environment.yaml We can use this environment in our snakemake rule as well! SAMPLES = [\"SRR1976948\"] rule all: input: expand(\"quality/{sample}_1.trim.fastq.gz\", sample=SAMPLES) expand(\"quality/{sample}_2.trim.fastq.gz\", sample=SAMPLES) rule trim_reads: input: in1=\"raw_data/{sample}_1.fastq.gz\", in2=\"raw_data/{sample}_2.fastq.gz\", output: out1=\"quality/{sample}_1.trim.fastq.gz\", out2=\"quality/{sample}_2.trim.fastq.gz\", json=\"quality/{sample}.fastp.json\", html=\"quality/{sample}.fastp.html\" conda: \"dib_rotation_environment.yaml\" shell: \"\"\" fastp --in1 {input.in1} --in2 {input.in2} \\ --out1 {output.out1} --out2 {output.out2} \\ --detect_adapter_for_pe --qualified_quality_phred 4 \\ --length_required 31 --correction \\ --json {output.json} --html {output.html} \"\"\" Here, we just have a single environment, so it was pretty easy to just run the Snakefile while within our dib_rotation environment. Using conda environment with snakemake becomes more useful as you use more tools, because it helps to keep different tools (which likely have different software dependencies) in separate conda environments. Run snakemake with --use-conda to have snakemake use the conda environment for this step. snakemake -j 1 --use-conda Why Automate with Workflow Systems? Workflow systems contain powerful infrastructure for workflow management that can coordinate runtime behavior, self-monitor progress and resource usage, and compile reports documenting the results of a workow. These features ensure that the steps for data analysis are minimally documented and repeatable from start to finish. When paired with proper software management, fully-contained workows are scalable, robust to software updates, and executable across platforms, meaning they will likely still execute the same set of commands with little investment by the user after weeks, months, or years. Check out our workflows preprint for a guide. Bonus exercise The above exercise walks you through how to write and run a snakefile for quality trimming. For additional practice using snakemake, snakemake the entire rotation project (e.g., running sourmash and spacegraphcats). As a super extra bonus, you can even snakemake the data download step. Note you will need two conda environment yaml file, one for the spacegraphcats rule and one for the rest of the rules.","title":"Automation, Workflows, and Repeatability"},{"location":"11_workflows_and_repeatability/#workflows-automation-and-repeatability","text":"For everything we have done so far, we have copied and pasted a lot of commands to accomplish what we want. This works! But can also be time consuming, and is more prone to error. We will show you next how to put all of these commands into a shell script. A shell script is a text file full of shell commands, that run just as if you're running them interactively at the command line.","title":"Workflows, Automation, and Repeatability"},{"location":"11_workflows_and_repeatability/#writing-a-shell-script","text":"Let's put some of our commands from the quality trimming module into one script. We'll call it run_qc.sh . The sh at the end of the tells you that this is a bash script. -First, cd into the 2020_rotation_project directory cd ~/2020_rotation_project Now, use nano to create and edit a file called run-qc.sh nano run-qc.sh will open the file. Now add the following text: cd ~/2020_rotation_project mkdir -p quality cd quality ln -s ~/2020_rotation_project/raw_data/*.fastq.gz ./ printf \"I see $(ls -1 *.fastq.gz | wc -l) files here.\\n\" for infile in *_1.fastq.gz do name=$(basename ${infile} _1.fastq.gz) fastp --in1 ${name}_1.fastq.gz --in2 ${name}_2.fastq.gz --out1 ${name}_1.trim.fastq.gz --out2 ${name}_2.trim.fastq.gz --detect_adapter_for_pe \\ --qualified_quality_phred 4 --length_required 31 --correction --json ${name}.trim.json --html ${name}.trim.html done This is now a shell script that you can use to execute all of those commands in one go, including running fastp on all six samples! Exit nano and try it out! Run: cd ~/2020_rotation_project bash run-qc.sh","title":"Writing a shell script"},{"location":"11_workflows_and_repeatability/#re-running-the-shell-script","text":"Suppose you wanted to re-run the script. How would you do that? Well, note that the quality directory is created at the top of the script, and everything is executed in that directory. So if you remove the quality directory like so, rm -rf quality The -rf here means that you'd like to remove the whole directory \"recursively\" ( r ) and that you'd like file deltion to happen without asking for permission for each file ( f ) You can then do: bash run-qc.sh","title":"Re-running the shell script"},{"location":"11_workflows_and_repeatability/#some-tricks-for-writing-shell-scripts","text":"","title":"Some tricks for writing shell scripts"},{"location":"11_workflows_and_repeatability/#make-it-executable","text":"You can get rid of the bash part of the command above with some magic: Put #! /bin/bash at the top of the file, and then run chmod +x ~/2020_rotation_project/run-qc.sh at the command line. You can now run ./run-qc.sh instead of bash run-qc.sh . You might be thinking, ok, why is this important? Well, you can do the same with R scripts and Python scripts (but put /usr/bin/env Rscript or /usr/bin/env python at the top, instead of /bin/bash ). This basically annotates the script with the language it's written in, so you don't have to know or remember yourself. So: it's not necessary but it's a nice trick. You can also always force a script to be run in a particular language by specifying bash <scriptname> or Rscript <Scriptname> , too.","title":"Make it executable"},{"location":"11_workflows_and_repeatability/#automation-with-workflow-systems","text":"Automation via shell script is wonderful, but there are a few problems here. First, you have to run the entire workflow each time and it recomputes everything every time. If you're running a workflow that takes 4 days, and you change a command at the end, you'll have to manually go in and just run the stuff that depends on the changed command. Second, it's very explicit and not very generalizable . If you want to run it on a different dataset, you're going to have to change a lot of commands. You can read more about using workflow systems to streamline data-intensive biology in our preprint here .","title":"Automation with Workflow Systems!"},{"location":"11_workflows_and_repeatability/#snakemake","text":"Snakemake is one of several workflow systems that help solve these problems. If you want to learn snakemake, we recommend working through a tutorial, such as the one here . It's also worth checking out the snakemake documentation here . Here, we'll demo how to run the same steps above, but in Snakemake. First, let's install snakemake in our conda environment: conda install -y snakemake-minimal We're going to automate the same set of commands for trimming, but in snakemake. Open a file called Snakefile using nano : nano Snakefile Here is the command we would need for a single sample, SRR1976948 rule all: input: \"quality/SRR1976948_1.trim.fastq.gz\", \"quality/SRR1976948_2.trim.fastq.gz\" rule trim_reads: input: in1=\"raw_data/SRR1976948_1.fastq.gz\", in2=\"raw_data/SRR1976948_2.fastq.gz\", output: out1=\"quality/SRR1976948_1.trim.fastq.gz\", out2=\"quality/SRR1976948_2.trim.fastq.gz\", json=\"quality/SRR1976948.fastp.json\", html=\"quality/SRR1976948.fastp.html\" shell: \"\"\" fastp --in1 {input.in1} --in2 {input.in2} \\ --out1 {output.out1} --out2 {output.out2} \\ --detect_adapter_for_pe --qualified_quality_phred 4 \\ --length_required 31 --correction \\ --json {output.json} --html {output.html} \"\"\" We can run it like this: snakemake -n the -n tells snakemake to run a \"dry run\" - that is, just check that the input files exist and all files specified in rule all can be created from the rules provided within the Snakefile). you should see \"Nothing to be done.\" That's because the trimmed files already exist! Let's fix that: rm quality/SRR1976948*.trim.fastq.gz and now, when you run snakemake , you should see the fastp being run. Yay w00t! Then if you run snakemake again, you will see that it doesn't need to do anything - all the files are \"up to date\".","title":"Snakemake"},{"location":"11_workflows_and_repeatability/#running-all-files-at-once","text":"Snakemake wouldn't be very useful if it could only trim one file at a time, so let's modify the Snakefile so it could run more files at once: SAMPLES = [\"SRR1976948\"] rule all: input: expand(\"quality/{sample}_1.trim.fastq.gz\", sample=SAMPLES) expand(\"quality/{sample}_2.trim.fastq.gz\", sample=SAMPLES) rule trim_reads: input: in1=\"raw_data/{sample}_R1.fastq.gz\", in2=\"raw_data/{sample}_R2.fastq.gz\", output: out1=\"quality/{sample}_1.trim.fastq.gz\", out2=\"quality/{sample}_2.trim.fastq.gz\", json=\"quality/{sample}.fastp.json\", html=\"quality/{sample}.fastp.html\" shell: \"\"\" fastp --in1 {input.in1} --in2 {input.in2} \\ --out1 {output.out1} --out2 {output.out2} \\ --detect_adapter_for_pe --qualified_quality_phred 4 \\ --length_required 31 --correction \\ --json {output.json} --html {output.html} \"\"\" Try another dryrun: snakemake -n Now actually run the workflow: snakemake -j 1 the -j 1 tells snakemake to run a single job at a time. You can increase this number if you have access to more cpu (e.g. you're in an srun session where you asked for more cpu with the -n parameter). Again, we see there's nothing to be done - the files exist! Try removing the quality trimmed files and running again. rm quality/*.trim.fastq.gz","title":"Running all files at once"},{"location":"11_workflows_and_repeatability/#adding-an-environment","text":"We've been using a conda environment throughout our modules. We can export the installed package names to a file that we can use to re-install all packages in a single step (like on a different computer). conda env export -n dib_rotation -f ~/2020_rotation_project/dib_rotation_environment.yaml We can use this environment in our snakemake rule as well! SAMPLES = [\"SRR1976948\"] rule all: input: expand(\"quality/{sample}_1.trim.fastq.gz\", sample=SAMPLES) expand(\"quality/{sample}_2.trim.fastq.gz\", sample=SAMPLES) rule trim_reads: input: in1=\"raw_data/{sample}_1.fastq.gz\", in2=\"raw_data/{sample}_2.fastq.gz\", output: out1=\"quality/{sample}_1.trim.fastq.gz\", out2=\"quality/{sample}_2.trim.fastq.gz\", json=\"quality/{sample}.fastp.json\", html=\"quality/{sample}.fastp.html\" conda: \"dib_rotation_environment.yaml\" shell: \"\"\" fastp --in1 {input.in1} --in2 {input.in2} \\ --out1 {output.out1} --out2 {output.out2} \\ --detect_adapter_for_pe --qualified_quality_phred 4 \\ --length_required 31 --correction \\ --json {output.json} --html {output.html} \"\"\" Here, we just have a single environment, so it was pretty easy to just run the Snakefile while within our dib_rotation environment. Using conda environment with snakemake becomes more useful as you use more tools, because it helps to keep different tools (which likely have different software dependencies) in separate conda environments. Run snakemake with --use-conda to have snakemake use the conda environment for this step. snakemake -j 1 --use-conda","title":"Adding an environment"},{"location":"11_workflows_and_repeatability/#why-automate-with-workflow-systems","text":"Workflow systems contain powerful infrastructure for workflow management that can coordinate runtime behavior, self-monitor progress and resource usage, and compile reports documenting the results of a workow. These features ensure that the steps for data analysis are minimally documented and repeatable from start to finish. When paired with proper software management, fully-contained workows are scalable, robust to software updates, and executable across platforms, meaning they will likely still execute the same set of commands with little investment by the user after weeks, months, or years. Check out our workflows preprint for a guide.","title":"Why Automate with Workflow Systems?"},{"location":"11_workflows_and_repeatability/#bonus-exercise","text":"The above exercise walks you through how to write and run a snakefile for quality trimming. For additional practice using snakemake, snakemake the entire rotation project (e.g., running sourmash and spacegraphcats). As a super extra bonus, you can even snakemake the data download step. Note you will need two conda environment yaml file, one for the spacegraphcats rule and one for the rest of the rules.","title":"Bonus exercise"},{"location":"12_angus_github/","text":"Version Control with Github Learning objectives Learn about version Control Learn about Github repositories Create local repositories Backup your work online using git Setup You\u2019ll need to sign up for a free account on GitHub.com . It\u2019s as simple as signing up for any other social network. Keep the email you picked handy; we\u2019ll be referencing it again in the lesson. Git is installed on many system, but if you don't already have it, instructions to install Git for Windows, Mac or Linux can be found here . What is Github? GitHub is a code hosting platform for version control and collaboration. It lets you and others work together on projects from anywhere. GitHub is now the largest online storage space of collaborative works that exists in the world What Is Git? Why use something like Git? Say you and a coworker are both updating pages on the same website. You make your changes, save them, and upload them back to the website. So far, so good. The problem comes when your coworker is working on the same page as you at the same time. One of you is about to have your work overwritten and erased. A version control application like Git keeps that from happening. You and your coworker can each upload your revisions to the same page, and Git will save two copies. Later, you can merge your changes together without losing any work along the way. You can even revert to an earlier version at any time, because Git keeps a \u201csnapshot\u201d of every change ever made. Git terms Repository: A directory or storage space where your projects can live. Sometimes GitHub users shorten this to \u201crepo.\u201d It can be local to a folder on your computer, or it can be a storage space on GitHub or another online host. You can keep code files, text files, image files, you name it, inside a repository. Version Control: Basically, the purpose Git was designed to serve. When you have a Microsoft Word file, you either overwrite every saved file with a new save, or you save multiple versions. With Git, you don\u2019t have to. It keeps \u201csnapshots\u201d of every point in time in the project\u2019s history, so you can never lose or overwrite it. Commit: This is the command that gives Git its power. When you commit, you are taking a \u201csnapshot\u201d of your repository at that point in time, giving you a checkpoint to which you can reevaluate or restore your project to any previous state. Branch: How do multiple people work on a project at the same time without Git getting them confused? Usually, they \u201cbranch off\u201d of the main project with their own versions full of changes they themselves have made. After they\u2019re done, it\u2019s time to \u201cmerge\u201d that branch back with the \u201cmaster,\u201d the main directory of the project. Git-Specific Commands git init : Initializes a new Git repository. Until you run this command inside a repository or directory, it\u2019s just a regular folder. Only after you input this does it accept further Git commands. git config : Short for \u201cconfigure,\u201d this is most useful when you\u2019re setting up Git for the first time. git help : Forgot a command? Type this into the command line to bring up the 21 most common git commands. You can also be more specific and type \u201cgit help init\u201d or another term to figure out how to use and configure a specific git command. git status : Check the status of your repository. See which files are inside it, which changes still need to be committed, and which branch of the repository you\u2019re currently working on. git add : This does not add new files to your repository. Instead, it brings new files to Git\u2019s attention. After you add files, they\u2019re included in Git\u2019s \u201csnapshots\u201d of the repository. git commit : Git\u2019s most important command. After you make any sort of change, you input this in order to take a \u201csnapshot\u201d of the repository. Usually it goes git commit -m \u201cMessage here.\u201d The -m indicates that the following section of the command should be read as a message. git branch : Working with multiple collaborators and want to make changes on your own? This command will let you build a new branch, or timeline of commits, of changes and file additions that are completely your own. Your title goes after the command. If you wanted a new branch called \u201ccats,\u201d you\u2019d type git branch cats . git checkout : Literally allows you to \u201ccheck out\u201d a repository that you are not currently inside. This is a navigational command that lets you move to the repository you want to check. You can use this command as g it checkout master to look at the master branch, or git checkout cats to look at another branch. git merge : When you\u2019re done working on a branch, you can merge your changes back to the master branch, which is visible to all collaborators. git merge cats would take all the changes you made to the \u201ccats\u201d branch and add them to the master. git push : If you\u2019re working on your local computer, and want your commits to be visible online on GitHub as well, you \u201cpush\u201d the changes up to GitHub with this command. git pull : If you\u2019re working on your local computer and want the most up-to-date version of your repository to work with, you \u201cpull\u201d the changes down from GitHub with this command. Setting Up GitHub And Git For The First Time It\u2019s time to introduce yourself to Git. Type in the following code: git config --global user.name \"Your Name Here\" Next, tell it your email and make sure it\u2019s the same email you used when you signed up for a GitHub.com account git config --global user.email \"your_email@youremail.com\" Creating Your Online Repository Now that you\u2019re all set up, it\u2019s time to create a place for your project to live. Both Git and GitHub refer to this as a repository, or \u201crepo\u201d for short, a digital directory or storage space where you can access your project, its files, and all the versions of its files that Git saves. On your Github profile, click the plus button and select a \"New Repository\". Give your repository a name & fill out the necessary information for your repository to be distinct and recognizeable. Don\u2019t worry about clicking the checkbox next to \u201cInitialize this repository with a README.\u201d A Readme file is usually a text file that explains a bit about the project. But we can make our own Readme file locally for practice. Click the green \u201cCreate Repository\u201d button and you\u2019re set. You now have an online space for your project to live in. Creating Your Local Repository To begin, let's create a new directory called MyProject. mkdir ~/MyProject Then we will move into this new directory. cd ~/MyProject To create a local repository, we will first initiate a new repository for \"MyProject\" by entering the following command: git init touch is a multi-purpose command, but one of its key uses is to creat new, empty files. In our case, we will create a new file called Readme.txt. touch Readme.txt We can check the status of our new repository by using git status . git status When we want Git to track a file, we use git add followed by the file we want Git to \"see\". If we do not use git add , Git will not \"see\" this file. git add Readme.txt Lastly, to have Git track the current \"snapshot\" of our file, we enter git commit . The -m flag allows us to add a personal message with the files we are committing. In the following example, our message is \"Add Readme.txt\". Examples of other messages could include version information, changes made to a document, document descriptions, etc. git commit -m \u201cAdd Readme.txt\u201d Now Git has a \"snapshot\" of this version of Readme.txt which you can return to at any time in the future! Connect Your Local Repository To Your GitHub Repository Online This setup also makes it easy to have multiple collaborators working on the same project. Each of you can work alone on your own computers, but upload or \u201cpush\u201d your changes up to the GitHub repository when they\u2019re ready. To tell Git the address off your remote repo in Github, Type the following replacing the address of the repo with your own git remote add origin https://github.com/username/myproject.git Git now knows there\u2019s a remote repository and it\u2019s where you want your local repository changes to go. To confirm, type this to check: git remote -v Great, Git is able to connect with our remote on Github. So, let's go ahead and push our files to Github git push origin master You will be prompted for your Github username and password at this point and you can see some output like this that git is sending packets of data to your github repo and by this you will force git to back up all of your commits since the last time you pushed to be backed up online. FOR FREE! Counting objects: 3, done. Writing objects: 100% (3/3), 217 bytes | 217.00 KiB/s, done. Total 3 (delta 0), reused 0 (delta 0) To https://github.com/sateeshbio5/angus_test.git * [new branch] master -> master Note: To avoid having to type your username and password each time you push/pull from your github repos, read about Secure Login here Collaborating via GitHub GitHub Issues: Issues are a great way to keep track of tasks, enhancements, and bugs for your projects. They\u2019re kind of like email\u2014except they can be shared and discussed with all. Read more about Mastering Issues on Github here GitHub Pull-Requests: Pull requests let you tell others about changes you've pushed to a branch in a repository on GitHub. Once a pull request is opened, you can discuss and review the potential changes with collaborators and add follow-up commits before your changes are merged into the base branch. Look at others' repositories: Hadley Wickham (ggplot2) Yihui Xie (knitr) ANGUS 2019 Host Websites & Blogs on GitHub GitHub Pages is an awesome feature that lets you host websites/blogs for you and your projects. Hosted directly from your GitHub repository. Just edit, push, and your changes are live. Read more about GitHub Pages here Sources for this tutorial & Additional Git Resources Introductory tutorial by Lauren Orsini here Pro Git Try Git Github Guides Github Reference Git - Simple Guide Github Hello World this lesson adapted from materials in ANGUS 2019","title":"Version Control with Git and GitHub"},{"location":"12_angus_github/#version-control-with-github","text":"Learning objectives Learn about version Control Learn about Github repositories Create local repositories Backup your work online using git","title":"Version Control with Github"},{"location":"12_angus_github/#setup","text":"You\u2019ll need to sign up for a free account on GitHub.com . It\u2019s as simple as signing up for any other social network. Keep the email you picked handy; we\u2019ll be referencing it again in the lesson. Git is installed on many system, but if you don't already have it, instructions to install Git for Windows, Mac or Linux can be found here .","title":"Setup"},{"location":"12_angus_github/#what-is-github","text":"GitHub is a code hosting platform for version control and collaboration. It lets you and others work together on projects from anywhere. GitHub is now the largest online storage space of collaborative works that exists in the world","title":"What is Github?"},{"location":"12_angus_github/#what-is-git","text":"Why use something like Git? Say you and a coworker are both updating pages on the same website. You make your changes, save them, and upload them back to the website. So far, so good. The problem comes when your coworker is working on the same page as you at the same time. One of you is about to have your work overwritten and erased. A version control application like Git keeps that from happening. You and your coworker can each upload your revisions to the same page, and Git will save two copies. Later, you can merge your changes together without losing any work along the way. You can even revert to an earlier version at any time, because Git keeps a \u201csnapshot\u201d of every change ever made.","title":"What Is Git?"},{"location":"12_angus_github/#git-terms","text":"","title":"Git terms"},{"location":"12_angus_github/#repository","text":"A directory or storage space where your projects can live. Sometimes GitHub users shorten this to \u201crepo.\u201d It can be local to a folder on your computer, or it can be a storage space on GitHub or another online host. You can keep code files, text files, image files, you name it, inside a repository.","title":"Repository:"},{"location":"12_angus_github/#version-control","text":"Basically, the purpose Git was designed to serve. When you have a Microsoft Word file, you either overwrite every saved file with a new save, or you save multiple versions. With Git, you don\u2019t have to. It keeps \u201csnapshots\u201d of every point in time in the project\u2019s history, so you can never lose or overwrite it.","title":"Version Control:"},{"location":"12_angus_github/#commit","text":"This is the command that gives Git its power. When you commit, you are taking a \u201csnapshot\u201d of your repository at that point in time, giving you a checkpoint to which you can reevaluate or restore your project to any previous state.","title":"Commit:"},{"location":"12_angus_github/#branch","text":"How do multiple people work on a project at the same time without Git getting them confused? Usually, they \u201cbranch off\u201d of the main project with their own versions full of changes they themselves have made. After they\u2019re done, it\u2019s time to \u201cmerge\u201d that branch back with the \u201cmaster,\u201d the main directory of the project.","title":"Branch:"},{"location":"12_angus_github/#git-specific-commands","text":"git init : Initializes a new Git repository. Until you run this command inside a repository or directory, it\u2019s just a regular folder. Only after you input this does it accept further Git commands. git config : Short for \u201cconfigure,\u201d this is most useful when you\u2019re setting up Git for the first time. git help : Forgot a command? Type this into the command line to bring up the 21 most common git commands. You can also be more specific and type \u201cgit help init\u201d or another term to figure out how to use and configure a specific git command. git status : Check the status of your repository. See which files are inside it, which changes still need to be committed, and which branch of the repository you\u2019re currently working on. git add : This does not add new files to your repository. Instead, it brings new files to Git\u2019s attention. After you add files, they\u2019re included in Git\u2019s \u201csnapshots\u201d of the repository. git commit : Git\u2019s most important command. After you make any sort of change, you input this in order to take a \u201csnapshot\u201d of the repository. Usually it goes git commit -m \u201cMessage here.\u201d The -m indicates that the following section of the command should be read as a message. git branch : Working with multiple collaborators and want to make changes on your own? This command will let you build a new branch, or timeline of commits, of changes and file additions that are completely your own. Your title goes after the command. If you wanted a new branch called \u201ccats,\u201d you\u2019d type git branch cats . git checkout : Literally allows you to \u201ccheck out\u201d a repository that you are not currently inside. This is a navigational command that lets you move to the repository you want to check. You can use this command as g it checkout master to look at the master branch, or git checkout cats to look at another branch. git merge : When you\u2019re done working on a branch, you can merge your changes back to the master branch, which is visible to all collaborators. git merge cats would take all the changes you made to the \u201ccats\u201d branch and add them to the master. git push : If you\u2019re working on your local computer, and want your commits to be visible online on GitHub as well, you \u201cpush\u201d the changes up to GitHub with this command. git pull : If you\u2019re working on your local computer and want the most up-to-date version of your repository to work with, you \u201cpull\u201d the changes down from GitHub with this command.","title":"Git-Specific Commands"},{"location":"12_angus_github/#setting-up-github-and-git-for-the-first-time","text":"It\u2019s time to introduce yourself to Git. Type in the following code: git config --global user.name \"Your Name Here\" Next, tell it your email and make sure it\u2019s the same email you used when you signed up for a GitHub.com account git config --global user.email \"your_email@youremail.com\"","title":"Setting Up GitHub And Git For The First Time"},{"location":"12_angus_github/#creating-your-online-repository","text":"Now that you\u2019re all set up, it\u2019s time to create a place for your project to live. Both Git and GitHub refer to this as a repository, or \u201crepo\u201d for short, a digital directory or storage space where you can access your project, its files, and all the versions of its files that Git saves. On your Github profile, click the plus button and select a \"New Repository\". Give your repository a name & fill out the necessary information for your repository to be distinct and recognizeable. Don\u2019t worry about clicking the checkbox next to \u201cInitialize this repository with a README.\u201d A Readme file is usually a text file that explains a bit about the project. But we can make our own Readme file locally for practice. Click the green \u201cCreate Repository\u201d button and you\u2019re set. You now have an online space for your project to live in.","title":"Creating Your Online Repository"},{"location":"12_angus_github/#creating-your-local-repository","text":"To begin, let's create a new directory called MyProject. mkdir ~/MyProject Then we will move into this new directory. cd ~/MyProject To create a local repository, we will first initiate a new repository for \"MyProject\" by entering the following command: git init touch is a multi-purpose command, but one of its key uses is to creat new, empty files. In our case, we will create a new file called Readme.txt. touch Readme.txt We can check the status of our new repository by using git status . git status When we want Git to track a file, we use git add followed by the file we want Git to \"see\". If we do not use git add , Git will not \"see\" this file. git add Readme.txt Lastly, to have Git track the current \"snapshot\" of our file, we enter git commit . The -m flag allows us to add a personal message with the files we are committing. In the following example, our message is \"Add Readme.txt\". Examples of other messages could include version information, changes made to a document, document descriptions, etc. git commit -m \u201cAdd Readme.txt\u201d Now Git has a \"snapshot\" of this version of Readme.txt which you can return to at any time in the future!","title":"Creating Your Local Repository"},{"location":"12_angus_github/#connect-your-local-repository-to-your-github-repository-online","text":"This setup also makes it easy to have multiple collaborators working on the same project. Each of you can work alone on your own computers, but upload or \u201cpush\u201d your changes up to the GitHub repository when they\u2019re ready. To tell Git the address off your remote repo in Github, Type the following replacing the address of the repo with your own git remote add origin https://github.com/username/myproject.git Git now knows there\u2019s a remote repository and it\u2019s where you want your local repository changes to go. To confirm, type this to check: git remote -v Great, Git is able to connect with our remote on Github. So, let's go ahead and push our files to Github git push origin master You will be prompted for your Github username and password at this point and you can see some output like this that git is sending packets of data to your github repo and by this you will force git to back up all of your commits since the last time you pushed to be backed up online. FOR FREE! Counting objects: 3, done. Writing objects: 100% (3/3), 217 bytes | 217.00 KiB/s, done. Total 3 (delta 0), reused 0 (delta 0) To https://github.com/sateeshbio5/angus_test.git * [new branch] master -> master Note: To avoid having to type your username and password each time you push/pull from your github repos, read about Secure Login here","title":"Connect Your Local Repository To Your GitHub Repository Online"},{"location":"12_angus_github/#collaborating-via-github","text":"GitHub Issues: Issues are a great way to keep track of tasks, enhancements, and bugs for your projects. They\u2019re kind of like email\u2014except they can be shared and discussed with all. Read more about Mastering Issues on Github here GitHub Pull-Requests: Pull requests let you tell others about changes you've pushed to a branch in a repository on GitHub. Once a pull request is opened, you can discuss and review the potential changes with collaborators and add follow-up commits before your changes are merged into the base branch. Look at others' repositories: Hadley Wickham (ggplot2) Yihui Xie (knitr) ANGUS 2019","title":"Collaborating via GitHub"},{"location":"12_angus_github/#host-websites-blogs-on-github","text":"GitHub Pages is an awesome feature that lets you host websites/blogs for you and your projects. Hosted directly from your GitHub repository. Just edit, push, and your changes are live. Read more about GitHub Pages here","title":"Host Websites &amp; Blogs on GitHub"},{"location":"12_angus_github/#sources-for-this-tutorial-additional-git-resources","text":"Introductory tutorial by Lauren Orsini here Pro Git Try Git Github Guides Github Reference Git - Simple Guide Github Hello World this lesson adapted from materials in ANGUS 2019","title":"Sources for this tutorial &amp; Additional Git Resources"},{"location":"13_contribute_on_github/","text":"How to edit the lessons in this webpage: Edit these lessons on GitHub If you can't fix an error or make an addition right now, file an issue with the doc that needs changing. Each doc (shows up as a tab or page on the website) is stored as a .md document within the doc folder. Creating a pull request: git clone https://github.com/dib-lab/dib_rotation.git to clone the repo on to you computer from inside that repo, make yourself a branch to work on: git checkout -b my-awesome-branch make some changes in the markdown on your branch git add changed_file.md for each file you change git commit to save changes When you're satisfied with your changes, open a pull request to integrate these changes into the main branch: git push --set-upstream origin my-awesome-branch git push to upload updated files (all changes up to last commit) when you open GitHub again in a browser, it will prompt you to open a pull request if you are correcting a filed issue, it's helpful to tag the issue number in the pull request (e.g. fixes issue #1234 ) When integrated, your changes will automatically render on the training website. Add a new page (markdown document): put the .md file in the doc folder add the title and file name to mkdocs.yml under the nav section, formatted with the name and title like the other docs are this allows a tab to render for your doc in the side bar on the webpage place it in the list in the place you want it to appear on the webpage View the updated rendered website locally: make a conda env mamba create -n mkdocs mkdocs if you dont have mamba, use conda create -n mkdocs mkdocs activate that environment: conda activate mkdocs pip install mkdocs-material to install the \"material\" theme cd to the repo ( dib_rotation , or wherever mkdocs.yml is sitting) build the docs into a website: mkdocs build serves the site to a local webserver: mkdocs serve a browser window might open automatically, this may not work if you are using wsl the address should also be visible on the terminal output if you need to input it manually. the URL is short, looks something like http://123.0.0.1:4000/","title":"Contribute on GitHub"},{"location":"13_contribute_on_github/#how-to-edit-the-lessons-in-this-webpage","text":"Edit these lessons on GitHub If you can't fix an error or make an addition right now, file an issue with the doc that needs changing. Each doc (shows up as a tab or page on the website) is stored as a .md document within the doc folder.","title":"How to edit the lessons in this webpage:"},{"location":"13_contribute_on_github/#creating-a-pull-request","text":"git clone https://github.com/dib-lab/dib_rotation.git to clone the repo on to you computer from inside that repo, make yourself a branch to work on: git checkout -b my-awesome-branch make some changes in the markdown on your branch git add changed_file.md for each file you change git commit to save changes When you're satisfied with your changes, open a pull request to integrate these changes into the main branch: git push --set-upstream origin my-awesome-branch git push to upload updated files (all changes up to last commit) when you open GitHub again in a browser, it will prompt you to open a pull request if you are correcting a filed issue, it's helpful to tag the issue number in the pull request (e.g. fixes issue #1234 ) When integrated, your changes will automatically render on the training website.","title":"Creating a pull request:"},{"location":"13_contribute_on_github/#add-a-new-page-markdown-document","text":"put the .md file in the doc folder add the title and file name to mkdocs.yml under the nav section, formatted with the name and title like the other docs are this allows a tab to render for your doc in the side bar on the webpage place it in the list in the place you want it to appear on the webpage","title":"Add a new page (markdown document):"},{"location":"13_contribute_on_github/#view-the-updated-rendered-website-locally","text":"make a conda env mamba create -n mkdocs mkdocs if you dont have mamba, use conda create -n mkdocs mkdocs activate that environment: conda activate mkdocs pip install mkdocs-material to install the \"material\" theme cd to the repo ( dib_rotation , or wherever mkdocs.yml is sitting) build the docs into a website: mkdocs build serves the site to a local webserver: mkdocs serve a browser window might open automatically, this may not work if you are using wsl the address should also be visible on the terminal output if you need to input it manually. the URL is short, looks something like http://123.0.0.1:4000/","title":"View the updated rendered website locally:"}]}